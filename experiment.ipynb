{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Experiment\n",
    "This notebook is used for tuning the hyper-parameters of pretrained models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# import method that can run model with option\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# 增加可读性\n",
    "from thop import profile, clever_format\n",
    "import models as module_arch\n",
    "import utils.data_loader as module_data\n",
    "import utils.loss_helper as module_loss\n",
    "import utils.metric as module_metric\n",
    "from run_model import run\n",
    "from utils.parse_config import ConfigParser\n",
    "from utils.tools import prepare_device\n",
    "# fix random seeds for reproducibility\n",
    "from utils.trainer import Trainer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# define the hyper-parameters here\n",
    "image_sizes = [128, 256, 512]\n",
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "batch_sizes = [16, 32, 64]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# define default training option here\n",
    "option = {\n",
    "    \"name\": \"googlenet\",\n",
    "    \"arch;args;pretrained_model\": \"googlenet\", # pretrained model name\n",
    "    \"arch;args;freeze_param\": 1, # freeze the parameters of pretrained model\n",
    "    \"data_loader;args;img_size\": 512,\n",
    "    \"data_loader;args;batch_size\": 32,\n",
    "    \"optimizer;args;lr\": 0.001\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# define evaluation result path\n",
    "result_path = \"saved/result\"\n",
    "os.makedirs(result_path, exist_ok=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 Loss: 0.6067: 100%|██████████| 176/176 [00:29<00:00,  5.87it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  5.79it/s]\n",
      "Train Epoch: 2 Loss: 0.5843: 100%|██████████| 176/176 [00:30<00:00,  5.80it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.08it/s]\n",
      "Train Epoch: 3 Loss: 0.6302: 100%|██████████| 176/176 [00:29<00:00,  6.06it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.08it/s]\n",
      "Train Epoch: 4 Loss: 0.5662: 100%|██████████| 176/176 [00:29<00:00,  5.99it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.05it/s]\n",
      "Train Epoch: 5 Loss: 0.5723: 100%|██████████| 176/176 [00:29<00:00,  6.00it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.00it/s]\n",
      "Train Epoch: 6 Loss: 0.7149: 100%|██████████| 176/176 [00:29<00:00,  5.93it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.01it/s]\n",
      "Train Epoch: 7 Loss: 0.6554: 100%|██████████| 176/176 [00:29<00:00,  5.93it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.00it/s]\n",
      "Train Epoch: 8 Loss: 0.7024: 100%|██████████| 176/176 [00:29<00:00,  5.92it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.01it/s]\n",
      "Train Epoch: 9 Loss: 0.5277: 100%|██████████| 176/176 [00:29<00:00,  5.89it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  5.97it/s]\n",
      "Train Epoch: 1 Loss: 0.638: 100%|██████████| 176/176 [00:40<00:00,  4.29it/s] \n",
      "100%|██████████| 20/20 [00:04<00:00,  4.39it/s]\n",
      "Train Epoch: 2 Loss: 0.5977: 100%|██████████| 176/176 [00:41<00:00,  4.20it/s]\n",
      "100%|██████████| 20/20 [00:05<00:00,  3.99it/s]\n",
      "Train Epoch: 3 Loss: 0.5834: 100%|██████████| 176/176 [00:42<00:00,  4.14it/s]\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.11it/s]\n",
      "Train Epoch: 4 Loss: 0.6327: 100%|██████████| 176/176 [00:42<00:00,  4.16it/s]\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.18it/s]\n",
      "Train Epoch: 5 Loss: 0.5682: 100%|██████████| 176/176 [00:42<00:00,  4.17it/s]\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.26it/s]\n",
      "Train Epoch: 6 Loss: 0.5721: 100%|██████████| 176/176 [00:41<00:00,  4.27it/s]\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.33it/s]\n",
      "Train Epoch: 7 Loss: 0.5323: 100%|██████████| 176/176 [00:41<00:00,  4.27it/s]\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.31it/s]\n",
      "Train Epoch: 8 Loss: 0.6647: 100%|██████████| 176/176 [00:42<00:00,  4.13it/s]\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.30it/s]\n",
      "Train Epoch: 1 Loss: 0.6741: 100%|██████████| 176/176 [01:34<00:00,  1.86it/s]\n",
      "100%|██████████| 20/20 [00:11<00:00,  1.81it/s]\n",
      "Train Epoch: 2 Loss: 0.6022: 100%|██████████| 176/176 [01:38<00:00,  1.78it/s]\n",
      "100%|██████████| 20/20 [00:10<00:00,  1.82it/s]\n",
      "Train Epoch: 3 Loss: 0.5914: 100%|██████████| 176/176 [01:39<00:00,  1.78it/s]\n",
      "100%|██████████| 20/20 [00:11<00:00,  1.80it/s]\n",
      "Train Epoch: 4 Loss: 0.5798: 100%|██████████| 176/176 [01:39<00:00,  1.76it/s]\n",
      "100%|██████████| 20/20 [00:10<00:00,  1.82it/s]\n",
      "Train Epoch: 5 Loss: 0.602: 100%|██████████| 176/176 [01:39<00:00,  1.78it/s] \n",
      "100%|██████████| 20/20 [00:10<00:00,  1.83it/s]\n",
      "Train Epoch: 6 Loss: 0.5533: 100%|██████████| 176/176 [01:38<00:00,  1.78it/s]\n",
      "100%|██████████| 20/20 [00:10<00:00,  1.83it/s]\n",
      "Train Epoch: 7 Loss: 0.5606: 100%|██████████| 176/176 [01:38<00:00,  1.78it/s]\n",
      "100%|██████████| 20/20 [00:11<00:00,  1.80it/s]\n",
      "Train Epoch: 8 Loss: 0.6132: 100%|██████████| 176/176 [01:42<00:00,  1.72it/s]\n",
      "100%|██████████| 20/20 [00:11<00:00,  1.76it/s]\n"
     ]
    }
   ],
   "source": [
    "trainers = []\n",
    "for image_size in image_sizes:\n",
    "    option.update({\"data_loader;args;img_size\": image_size})\n",
    "    run_id = f\"{image_size}_{option['data_loader;args;batch_size']}_{option['optimizer;args;lr']}\"\n",
    "    # close old logging process\n",
    "    logging.shutdown()\n",
    "    trainers.append(run(\"pretrained_model.json\", option, run_id, False))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def evaluate_result(train_results):\n",
    "    # define the dataframe of result\n",
    "    result_df = pd.DataFrame(columns=[\"model\", \"val_auc\", \"val_acc\", \"val_loss\", \"hyper-parameters\"])\n",
    "\n",
    "    for trainer in train_results:\n",
    "        config = trainer.config\n",
    "        log = trainer.log_best\n",
    "        param = f\"{config['data_loader']['args']['img_size']}_{config['data_loader']['args']['batch_size']}_{config['optimizer']['args']['lr']}\"\n",
    "        series = pd.Series({\n",
    "            \"model\": config[\"name\"], \"val_auc\": log[\"val_auc\"], \"val_acc\": log[\"val_accuracy\"],\n",
    "            \"hyper-parameters\": param, \"val_loss\": log[\"val_loss\"]\n",
    "        })\n",
    "        result_df = result_df.append(series, ignore_index=True)\n",
    "    return result_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "       model  val_auc  val_acc  val_loss hyper-parameters\n0  googlenet   0.6749   0.6322    0.6478     128_32_0.001\n1  googlenet   0.7291   0.6651    0.6180     256_32_0.001\n2  googlenet   0.7373   0.6541    0.6152     512_32_0.001",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>val_auc</th>\n      <th>val_acc</th>\n      <th>val_loss</th>\n      <th>hyper-parameters</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>googlenet</td>\n      <td>0.6749</td>\n      <td>0.6322</td>\n      <td>0.6478</td>\n      <td>128_32_0.001</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>googlenet</td>\n      <td>0.7291</td>\n      <td>0.6651</td>\n      <td>0.6180</td>\n      <td>256_32_0.001</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>googlenet</td>\n      <td>0.7373</td>\n      <td>0.6541</td>\n      <td>0.6152</td>\n      <td>512_32_0.001</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_size_result = evaluate_result(trainers)\n",
    "image_size_result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 Loss: 0.6061: 100%|██████████| 352/352 [00:41<00:00,  8.45it/s]\n",
      "100%|██████████| 40/40 [00:04<00:00,  8.52it/s]\n",
      "Train Epoch: 2 Loss: 0.6081: 100%|██████████| 352/352 [00:41<00:00,  8.38it/s]\n",
      "100%|██████████| 40/40 [00:04<00:00,  8.35it/s]\n",
      "Train Epoch: 3 Loss: 0.5214: 100%|██████████| 352/352 [00:42<00:00,  8.27it/s]\n",
      "100%|██████████| 40/40 [00:04<00:00,  8.19it/s]\n",
      "Train Epoch: 4 Loss: 0.5508: 100%|██████████| 352/352 [00:43<00:00,  8.07it/s]\n",
      "100%|██████████| 40/40 [00:04<00:00,  8.45it/s]\n",
      "Train Epoch: 5 Loss: 0.77: 100%|██████████| 352/352 [00:42<00:00,  8.31it/s]  \n",
      "100%|██████████| 40/40 [00:05<00:00,  8.00it/s]\n",
      "Train Epoch: 6 Loss: 0.4822: 100%|██████████| 352/352 [00:42<00:00,  8.26it/s]\n",
      "100%|██████████| 40/40 [00:04<00:00,  8.43it/s]\n",
      "Train Epoch: 7 Loss: 0.7284: 100%|██████████| 352/352 [00:42<00:00,  8.22it/s]\n",
      "100%|██████████| 40/40 [00:04<00:00,  8.37it/s]\n",
      "Train Epoch: 8 Loss: 0.5254: 100%|██████████| 352/352 [00:42<00:00,  8.21it/s]\n",
      "100%|██████████| 40/40 [00:04<00:00,  8.31it/s]\n",
      "Train Epoch: 9 Loss: 0.4807: 100%|██████████| 352/352 [00:43<00:00,  8.09it/s]\n",
      "100%|██████████| 40/40 [00:04<00:00,  8.09it/s]\n",
      "Train Epoch: 10 Loss: 0.5847: 100%|██████████| 352/352 [00:44<00:00,  7.90it/s]\n",
      "100%|██████████| 40/40 [00:04<00:00,  8.12it/s]\n",
      "Train Epoch: 11 Loss: 0.4963: 100%|██████████| 352/352 [00:43<00:00,  8.03it/s]\n",
      "100%|██████████| 40/40 [00:04<00:00,  8.23it/s]\n",
      "Train Epoch: 12 Loss: 0.6033: 100%|██████████| 352/352 [00:42<00:00,  8.19it/s]\n",
      "100%|██████████| 40/40 [00:04<00:00,  8.30it/s]\n",
      "Train Epoch: 13 Loss: 0.5405: 100%|██████████| 352/352 [00:42<00:00,  8.20it/s]\n",
      "100%|██████████| 40/40 [00:04<00:00,  8.35it/s]\n",
      "Train Epoch: 14 Loss: 0.4616: 100%|██████████| 352/352 [00:42<00:00,  8.20it/s]\n",
      "100%|██████████| 40/40 [00:04<00:00,  8.01it/s]\n",
      "Train Epoch: 1 Loss: 0.638: 100%|██████████| 176/176 [00:41<00:00,  4.21it/s] \n",
      "100%|██████████| 20/20 [00:04<00:00,  4.23it/s]\n",
      "Train Epoch: 2 Loss: 0.5977: 100%|██████████| 176/176 [00:41<00:00,  4.23it/s]\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.27it/s]\n",
      "Train Epoch: 3 Loss: 0.5834: 100%|██████████| 176/176 [00:41<00:00,  4.24it/s]\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.28it/s]\n",
      "Train Epoch: 4 Loss: 0.6327: 100%|██████████| 176/176 [00:41<00:00,  4.24it/s]\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.29it/s]\n",
      "Train Epoch: 5 Loss: 0.5682: 100%|██████████| 176/176 [00:41<00:00,  4.24it/s]\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.25it/s]\n",
      "Train Epoch: 6 Loss: 0.5721: 100%|██████████| 176/176 [00:42<00:00,  4.12it/s]\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.21it/s]\n",
      "Train Epoch: 7 Loss: 0.5323: 100%|██████████| 176/176 [00:42<00:00,  4.10it/s]\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.18it/s]\n",
      "Train Epoch: 8 Loss: 0.6647: 100%|██████████| 176/176 [00:42<00:00,  4.10it/s]\n",
      "100%|██████████| 20/20 [00:05<00:00,  3.85it/s]\n",
      "Train Epoch: 1 Loss: 0.6186: 100%|██████████| 88/88 [00:41<00:00,  2.12it/s]\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.09it/s]\n",
      "Train Epoch: 2 Loss: 0.5923: 100%|██████████| 88/88 [00:42<00:00,  2.07it/s]\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.05it/s]\n",
      "Train Epoch: 3 Loss: 0.5731: 100%|██████████| 88/88 [00:43<00:00,  2.00it/s]\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.01it/s]\n",
      "Train Epoch: 4 Loss: 0.6076: 100%|██████████| 88/88 [00:43<00:00,  2.01it/s]\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.04it/s]\n",
      "Train Epoch: 5 Loss: 0.5648: 100%|██████████| 88/88 [00:43<00:00,  2.00it/s]\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.03it/s]\n",
      "Train Epoch: 6 Loss: 0.5693: 100%|██████████| 88/88 [00:44<00:00,  1.97it/s]\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.05it/s]\n",
      "Train Epoch: 7 Loss: 0.5876: 100%|██████████| 88/88 [00:44<00:00,  1.98it/s]\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.06it/s]\n",
      "Train Epoch: 8 Loss: 0.5792: 100%|██████████| 88/88 [00:44<00:00,  1.97it/s]\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.90it/s]\n",
      "Train Epoch: 9 Loss: 0.5726: 100%|██████████| 88/88 [00:45<00:00,  1.95it/s]\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.98it/s]\n",
      "Train Epoch: 10 Loss: 0.5137: 100%|██████████| 88/88 [00:44<00:00,  1.96it/s]\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.98it/s]\n",
      "Train Epoch: 11 Loss: 0.5439: 100%|██████████| 88/88 [00:42<00:00,  2.05it/s]\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.11it/s]\n"
     ]
    }
   ],
   "source": [
    "# I choose image_size=256, because the time consuming is much lower then image_size=512, while the performance is still good enough.\n",
    "option.update({\"data_loader;args;img_size\": 256})\n",
    "trainers = []\n",
    "for bs in batch_sizes:\n",
    "    option.update({\"data_loader;args;batch_size\": bs})\n",
    "    run_id = f\"{option['data_loader;args;img_size']}_{bs}_{option['optimizer;args;lr']}\"\n",
    "    logging.shutdown()\n",
    "    trainers.append(run(\"pretrained_model.json\", option, run_id))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "       model  val_auc  val_acc  val_loss hyper-parameters\n0  googlenet   0.7315   0.6573    0.6178     256_16_0.001\n1  googlenet   0.7291   0.6651    0.6180     256_32_0.001\n2  googlenet   0.7363   0.6761    0.6125     256_64_0.001",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>val_auc</th>\n      <th>val_acc</th>\n      <th>val_loss</th>\n      <th>hyper-parameters</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>googlenet</td>\n      <td>0.7315</td>\n      <td>0.6573</td>\n      <td>0.6178</td>\n      <td>256_16_0.001</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>googlenet</td>\n      <td>0.7291</td>\n      <td>0.6651</td>\n      <td>0.6180</td>\n      <td>256_32_0.001</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>googlenet</td>\n      <td>0.7363</td>\n      <td>0.6761</td>\n      <td>0.6125</td>\n      <td>256_64_0.001</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs_result = evaluate_result(trainers)\n",
    "bs_result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 Loss: 0.6446: 100%|██████████| 88/88 [00:40<00:00,  2.17it/s]\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.11it/s]\n",
      "Train Epoch: 2 Loss: 0.579: 100%|██████████| 88/88 [00:43<00:00,  2.01it/s] \n",
      "100%|██████████| 10/10 [00:04<00:00,  2.01it/s]\n",
      "Train Epoch: 3 Loss: 0.7817: 100%|██████████| 88/88 [00:44<00:00,  2.00it/s]\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.05it/s]\n",
      "Train Epoch: 4 Loss: 0.7351: 100%|██████████| 88/88 [00:43<00:00,  2.04it/s]\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.05it/s]\n",
      "Train Epoch: 5 Loss: 0.6258: 100%|██████████| 88/88 [00:44<00:00,  1.98it/s]\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.98it/s]\n",
      "Train Epoch: 6 Loss: 0.6883: 100%|██████████| 88/88 [00:44<00:00,  1.98it/s]\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.94it/s]\n",
      "Train Epoch: 7 Loss: 0.7039: 100%|██████████| 88/88 [00:45<00:00,  1.93it/s]\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.99it/s]\n",
      "Train Epoch: 1 Loss: 0.6186: 100%|██████████| 88/88 [00:45<00:00,  1.95it/s]\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.03it/s]\n",
      "Train Epoch: 2 Loss: 0.5923: 100%|██████████| 88/88 [00:43<00:00,  2.00it/s]\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.05it/s]\n",
      "Train Epoch: 3 Loss: 0.5731: 100%|██████████| 88/88 [00:43<00:00,  2.02it/s]\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.04it/s]\n",
      "Train Epoch: 4 Loss: 0.6076: 100%|██████████| 88/88 [00:43<00:00,  2.00it/s]\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.04it/s]\n",
      "Train Epoch: 5 Loss: 0.5648: 100%|██████████| 88/88 [00:43<00:00,  2.02it/s]\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.05it/s]\n",
      "Train Epoch: 6 Loss: 0.5693: 100%|██████████| 88/88 [00:43<00:00,  2.01it/s]\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.99it/s]\n",
      "Train Epoch: 7 Loss: 0.5876: 100%|██████████| 88/88 [00:44<00:00,  1.99it/s]\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.02it/s]\n",
      "Train Epoch: 8 Loss: 0.5792: 100%|██████████| 88/88 [00:44<00:00,  2.00it/s]\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.99it/s]\n",
      "Train Epoch: 9 Loss: 0.5726: 100%|██████████| 88/88 [00:44<00:00,  1.96it/s]\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.00it/s]\n",
      "Train Epoch: 10 Loss: 0.5137: 100%|██████████| 88/88 [00:43<00:00,  2.02it/s]\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.05it/s]\n",
      "Train Epoch: 11 Loss: 0.5439: 100%|██████████| 88/88 [00:44<00:00,  1.98it/s]\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.95it/s]\n",
      "Train Epoch: 1 Loss: 0.6241: 100%|██████████| 88/88 [00:43<00:00,  2.02it/s]\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.05it/s]\n",
      "Train Epoch: 2 Loss: 0.5899: 100%|██████████| 88/88 [00:44<00:00,  1.96it/s]\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.94it/s]\n",
      "Train Epoch: 3 Loss: 0.6217: 100%|██████████| 88/88 [00:45<00:00,  1.95it/s]\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.93it/s]\n",
      "Train Epoch: 4 Loss: 0.6362: 100%|██████████| 88/88 [00:43<00:00,  2.00it/s]\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.09it/s]\n",
      "Train Epoch: 5 Loss: 0.612: 100%|██████████| 88/88 [00:44<00:00,  1.96it/s] \n",
      "100%|██████████| 10/10 [00:05<00:00,  1.92it/s]\n",
      "Train Epoch: 6 Loss: 0.5939: 100%|██████████| 88/88 [00:44<00:00,  1.98it/s]\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.94it/s]\n",
      "Train Epoch: 7 Loss: 0.6396: 100%|██████████| 88/88 [00:45<00:00,  1.92it/s]\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.93it/s]\n",
      "Train Epoch: 8 Loss: 0.5812: 100%|██████████| 88/88 [00:44<00:00,  1.98it/s]\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.06it/s]\n",
      "Train Epoch: 9 Loss: 0.6194: 100%|██████████| 88/88 [00:45<00:00,  1.93it/s]\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.98it/s]\n",
      "Train Epoch: 10 Loss: 0.5383: 100%|██████████| 88/88 [00:44<00:00,  1.97it/s]\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.99it/s]\n",
      "Train Epoch: 11 Loss: 0.6427: 100%|██████████| 88/88 [00:45<00:00,  1.96it/s]\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.01it/s]\n",
      "Train Epoch: 12 Loss: 0.6101: 100%|██████████| 88/88 [00:44<00:00,  1.96it/s]\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.99it/s]\n",
      "Train Epoch: 13 Loss: 0.5881: 100%|██████████| 88/88 [00:44<00:00,  1.97it/s]\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.90it/s]\n",
      "Train Epoch: 14 Loss: 0.5776: 100%|██████████| 88/88 [00:44<00:00,  1.99it/s]\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.03it/s]\n",
      "Train Epoch: 15 Loss: 0.5688: 100%|██████████| 88/88 [00:45<00:00,  1.95it/s]\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.99it/s]\n",
      "Train Epoch: 16 Loss: 0.5632: 100%|██████████| 88/88 [00:43<00:00,  2.02it/s]\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.99it/s]\n",
      "Train Epoch: 17 Loss: 0.5285: 100%|██████████| 88/88 [00:44<00:00,  2.00it/s]\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.06it/s]\n",
      "Train Epoch: 18 Loss: 0.5801: 100%|██████████| 88/88 [00:43<00:00,  2.02it/s]\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.03it/s]\n",
      "Train Epoch: 19 Loss: 0.6293: 100%|██████████| 88/88 [00:44<00:00,  1.98it/s]\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.97it/s]\n",
      "Train Epoch: 20 Loss: 0.5584: 100%|██████████| 88/88 [00:46<00:00,  1.89it/s]\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.04it/s]\n",
      "Train Epoch: 21 Loss: 0.5343: 100%|██████████| 88/88 [00:45<00:00,  1.95it/s]\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.88it/s]\n",
      "Train Epoch: 22 Loss: 0.4942: 100%|██████████| 88/88 [00:44<00:00,  1.99it/s]\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.06it/s]\n",
      "Train Epoch: 23 Loss: 0.5745: 100%|██████████| 88/88 [00:44<00:00,  1.99it/s]\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.90it/s]\n",
      "Train Epoch: 24 Loss: 0.5308: 100%|██████████| 88/88 [00:46<00:00,  1.91it/s]\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.99it/s]\n",
      "Train Epoch: 25 Loss: 0.5735: 100%|██████████| 88/88 [00:44<00:00,  2.00it/s]\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.04it/s]\n",
      "Train Epoch: 26 Loss: 0.5505: 100%|██████████| 88/88 [00:44<00:00,  1.97it/s]\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.96it/s]\n",
      "Train Epoch: 27 Loss: 0.5839: 100%|██████████| 88/88 [00:44<00:00,  1.97it/s]\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.94it/s]\n",
      "Train Epoch: 28 Loss: 0.5493: 100%|██████████| 88/88 [00:46<00:00,  1.90it/s]\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.06it/s]\n",
      "Train Epoch: 29 Loss: 0.5184: 100%|██████████| 88/88 [00:47<00:00,  1.87it/s]\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.88it/s]\n",
      "Train Epoch: 30 Loss: 0.5614: 100%|██████████| 88/88 [00:47<00:00,  1.85it/s]\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.86it/s]\n",
      "Train Epoch: 31 Loss: 0.5154: 100%|██████████| 88/88 [00:49<00:00,  1.78it/s]\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.80it/s]\n",
      "Train Epoch: 32 Loss: 0.5622: 100%|██████████| 88/88 [00:49<00:00,  1.77it/s]\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.82it/s]\n",
      "Train Epoch: 33 Loss: 0.5284: 100%|██████████| 88/88 [00:49<00:00,  1.77it/s]\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.86it/s]\n",
      "Train Epoch: 34 Loss: 0.5022: 100%|██████████| 88/88 [00:50<00:00,  1.75it/s]\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.78it/s]\n"
     ]
    }
   ],
   "source": [
    "# update with best batch_size: a larger batch size performs better\n",
    "option.update({\"data_loader;args;batch_size\": 64})\n",
    "trainers = []\n",
    "for lr in learning_rates:\n",
    "    option.update({\"optimizer;args;lr\": lr})\n",
    "    run_id = f\"{option['data_loader;args;img_size']}_{option['data_loader;args;batch_size']}_{lr}\"\n",
    "    logging.shutdown()\n",
    "    trainers.append(run(\"pretrained_model.json\", option, run_id))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "       model  val_auc  val_acc  val_loss hyper-parameters\n0  googlenet   0.6447   0.5947    0.7154      256_64_0.01\n1  googlenet   0.7363   0.6761    0.6125     256_64_0.001\n2  googlenet   0.7500   0.7027    0.6025    256_64_0.0001",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>val_auc</th>\n      <th>val_acc</th>\n      <th>val_loss</th>\n      <th>hyper-parameters</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>googlenet</td>\n      <td>0.6447</td>\n      <td>0.5947</td>\n      <td>0.7154</td>\n      <td>256_64_0.01</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>googlenet</td>\n      <td>0.7363</td>\n      <td>0.6761</td>\n      <td>0.6125</td>\n      <td>256_64_0.001</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>googlenet</td>\n      <td>0.7500</td>\n      <td>0.7027</td>\n      <td>0.6025</td>\n      <td>256_64_0.0001</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_result = evaluate_result(trainers)\n",
    "lr_result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "# update with best learning rate\n",
    "option.update({\"data_loader;args;lr\": 0.0001})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train ResNet, VGG, and DenseNet, and evaluate the results.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# define best suitable training option here\n",
    "option = {\n",
    "    \"arch;args;pretrained_model\": \"googlenet\", # pretrained model name\n",
    "    \"arch;args;freeze_param\": 1, # freeze the parameters of pretrained model\n",
    "    \"data_loader;args;img_size\": 256,\n",
    "    \"data_loader;args;batch_size\": 64,\n",
    "    \"optimizer;args;lr\": 0.0001\n",
    "}\n",
    "resnet_models = [\"resnet34\", \"resnet50\", \"resnet101\", \"resnet152\"]\n",
    "vgg_models = [\"vgg11_bn\", \"vgg13_bn\", \"vgg16_bn\", \"vgg19_bn\"]\n",
    "densnet_models = [\"densenet121\", \"densenet169\", \"densenet201\", \"densenet161\"]\n",
    "all_models = resnet_models+  vgg_models + densnet_models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run model: resnet34\n",
      "\n",
      "Run model: resnet50\n",
      "Run model: resnet101\n",
      "\n",
      "Run model: resnet152\n",
      "Run model: vgg11_bn\n",
      "\n",
      "Run model: vgg13_bn\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to C:\\Users\\Rui/.cache\\torch\\hub\\checkpoints\\resnet34-333f7ec4.pth\n",
      "Train Epoch: 1 Loss: 0.6329: 100%|██████████| 88/88 [00:45<00:00,  1.93it/s]\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.85it/s]\n",
      "Train Epoch: 2 Loss: 0.6131: 100%|██████████| 88/88 [00:51<00:00,  1.73it/s]\n",
      "100%|██████████| 10/10 [00:06<00:00,  1.63it/s]\n",
      "Train Epoch: 3 Loss: 0.6111: 100%|██████████| 88/88 [00:55<00:00,  1.59it/s]\n",
      "100%|██████████| 10/10 [00:06<00:00,  1.60it/s]\n",
      "Train Epoch: 4 Loss: 0.6073: 100%|██████████| 88/88 [00:56<00:00,  1.55it/s]\n",
      "100%|██████████| 10/10 [00:06<00:00,  1.62it/s]\n",
      "Train Epoch: 5 Loss: 0.5993: 100%|██████████| 88/88 [00:56<00:00,  1.54it/s]\n",
      "100%|██████████| 10/10 [00:06<00:00,  1.57it/s]\n",
      "Train Epoch: 6 Loss: 0.5591: 100%|██████████| 88/88 [00:56<00:00,  1.56it/s]\n",
      "100%|██████████| 10/10 [00:06<00:00,  1.54it/s]\n",
      "Train Epoch: 7 Loss: 0.6103: 100%|██████████| 88/88 [00:59<00:00,  1.48it/s]\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.39it/s]\n",
      "Train Epoch: 8 Loss: 0.6253: 100%|██████████| 88/88 [00:58<00:00,  1.51it/s]\n",
      "100%|██████████| 10/10 [00:06<00:00,  1.47it/s]\n",
      "Train Epoch: 9 Loss: 0.554: 100%|██████████| 88/88 [00:57<00:00,  1.52it/s] \n",
      "100%|██████████| 10/10 [00:06<00:00,  1.52it/s]\n",
      "Train Epoch: 10 Loss: 0.5656: 100%|██████████| 88/88 [00:59<00:00,  1.49it/s]\n",
      "100%|██████████| 10/10 [00:06<00:00,  1.43it/s]\n",
      "Train Epoch: 11 Loss: 0.5579: 100%|██████████| 88/88 [01:00<00:00,  1.45it/s]\n",
      "100%|██████████| 10/10 [00:06<00:00,  1.51it/s]\n",
      "Train Epoch: 12 Loss: 0.5077: 100%|██████████| 88/88 [00:58<00:00,  1.51it/s]\n",
      "100%|██████████| 10/10 [00:06<00:00,  1.53it/s]\n",
      "Train Epoch: 1 Loss: 0.6468: 100%|██████████| 88/88 [01:03<00:00,  1.38it/s]\n",
      "100%|██████████| 10/10 [00:06<00:00,  1.46it/s]\n",
      "Train Epoch: 2 Loss: 0.6502: 100%|██████████| 88/88 [01:02<00:00,  1.40it/s]\n",
      "100%|██████████| 10/10 [00:06<00:00,  1.48it/s]\n",
      "Train Epoch: 3 Loss: 0.6147: 100%|██████████| 88/88 [00:59<00:00,  1.47it/s]\n",
      "100%|██████████| 10/10 [00:06<00:00,  1.50it/s]\n",
      "Train Epoch: 4 Loss: 0.5306: 100%|██████████| 88/88 [01:00<00:00,  1.46it/s]\n",
      "100%|██████████| 10/10 [00:06<00:00,  1.47it/s]\n",
      "Train Epoch: 5 Loss: 0.5935: 100%|██████████| 88/88 [00:59<00:00,  1.47it/s]\n",
      "100%|██████████| 10/10 [00:06<00:00,  1.50it/s]\n",
      "Train Epoch: 6 Loss: 0.5622: 100%|██████████| 88/88 [00:59<00:00,  1.47it/s]\n",
      "100%|██████████| 10/10 [00:06<00:00,  1.51it/s]\n",
      "Train Epoch: 7 Loss: 0.6253: 100%|██████████| 88/88 [01:00<00:00,  1.45it/s]\n",
      "100%|██████████| 10/10 [00:06<00:00,  1.49it/s]\n",
      "Train Epoch: 8 Loss: 0.6014: 100%|██████████| 88/88 [01:01<00:00,  1.42it/s]\n",
      "100%|██████████| 10/10 [00:06<00:00,  1.47it/s]\n",
      "Train Epoch: 9 Loss: 0.592: 100%|██████████| 88/88 [01:00<00:00,  1.46it/s] \n",
      "100%|██████████| 10/10 [00:06<00:00,  1.49it/s]\n",
      "Train Epoch: 10 Loss: 0.553: 100%|██████████| 88/88 [00:59<00:00,  1.48it/s] \n",
      "100%|██████████| 10/10 [00:06<00:00,  1.53it/s]\n",
      "Train Epoch: 11 Loss: 0.5801: 100%|██████████| 88/88 [00:59<00:00,  1.49it/s]\n",
      "100%|██████████| 10/10 [00:06<00:00,  1.53it/s]\n",
      "Train Epoch: 12 Loss: 0.529: 100%|██████████| 88/88 [00:59<00:00,  1.47it/s] \n",
      "100%|██████████| 10/10 [00:06<00:00,  1.53it/s]\n",
      "Train Epoch: 13 Loss: 0.5785: 100%|██████████| 88/88 [00:58<00:00,  1.50it/s]\n",
      "100%|██████████| 10/10 [00:06<00:00,  1.54it/s]\n",
      "Train Epoch: 14 Loss: 0.5805: 100%|██████████| 88/88 [00:59<00:00,  1.49it/s]\n",
      "100%|██████████| 10/10 [00:06<00:00,  1.55it/s]\n",
      "Train Epoch: 15 Loss: 0.5272: 100%|██████████| 88/88 [00:59<00:00,  1.49it/s]\n",
      "100%|██████████| 10/10 [00:06<00:00,  1.55it/s]\n",
      "Train Epoch: 16 Loss: 0.5151: 100%|██████████| 88/88 [00:58<00:00,  1.49it/s]\n",
      "100%|██████████| 10/10 [00:06<00:00,  1.52it/s]\n",
      "Train Epoch: 17 Loss: 0.5101: 100%|██████████| 88/88 [00:59<00:00,  1.49it/s]\n",
      "100%|██████████| 10/10 [00:06<00:00,  1.52it/s]\n",
      "Downloading: \"https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\" to C:\\Users\\Rui/.cache\\torch\\hub\\checkpoints\\resnet101-5d3b4d8f.pth\n",
      "Train Epoch: 1 Loss: 0.6613: 100%|██████████| 88/88 [01:16<00:00,  1.15it/s]\n",
      "100%|██████████| 10/10 [00:08<00:00,  1.15it/s]\n",
      "Train Epoch: 2 Loss: 0.5593: 100%|██████████| 88/88 [01:18<00:00,  1.12it/s]\n",
      "100%|██████████| 10/10 [00:08<00:00,  1.15it/s]\n",
      "Train Epoch: 3 Loss: 0.6306: 100%|██████████| 88/88 [01:18<00:00,  1.12it/s]\n",
      "100%|██████████| 10/10 [00:08<00:00,  1.18it/s]\n",
      "Train Epoch: 4 Loss: 0.6318: 100%|██████████| 88/88 [01:18<00:00,  1.13it/s]\n",
      "100%|██████████| 10/10 [00:08<00:00,  1.13it/s]\n",
      "Train Epoch: 5 Loss: 0.5954: 100%|██████████| 88/88 [01:18<00:00,  1.13it/s]\n",
      "100%|██████████| 10/10 [00:08<00:00,  1.15it/s]\n",
      "Train Epoch: 6 Loss: 0.5615: 100%|██████████| 88/88 [01:18<00:00,  1.13it/s]\n",
      "100%|██████████| 10/10 [00:08<00:00,  1.17it/s]\n",
      "Train Epoch: 7 Loss: 0.5903: 100%|██████████| 88/88 [01:18<00:00,  1.13it/s]\n",
      "100%|██████████| 10/10 [00:08<00:00,  1.13it/s]\n",
      "Train Epoch: 8 Loss: 0.5335: 100%|██████████| 88/88 [01:18<00:00,  1.12it/s]\n",
      "100%|██████████| 10/10 [00:08<00:00,  1.16it/s]\n",
      "Train Epoch: 9 Loss: 0.5446: 100%|██████████| 88/88 [01:18<00:00,  1.13it/s]\n",
      "100%|██████████| 10/10 [00:08<00:00,  1.15it/s]\n",
      "Train Epoch: 10 Loss: 0.5945: 100%|██████████| 88/88 [01:17<00:00,  1.13it/s]\n",
      "100%|██████████| 10/10 [00:08<00:00,  1.22it/s]\n",
      "Train Epoch: 11 Loss: 0.5336: 100%|██████████| 88/88 [01:18<00:00,  1.13it/s]\n",
      "100%|██████████| 10/10 [00:08<00:00,  1.14it/s]\n",
      "Train Epoch: 12 Loss: 0.5533: 100%|██████████| 88/88 [01:17<00:00,  1.13it/s]\n",
      "100%|██████████| 10/10 [00:08<00:00,  1.15it/s]\n",
      "Train Epoch: 13 Loss: 0.5243: 100%|██████████| 88/88 [01:18<00:00,  1.13it/s]\n",
      "100%|██████████| 10/10 [00:08<00:00,  1.15it/s]\n",
      "Train Epoch: 14 Loss: 0.5499: 100%|██████████| 88/88 [01:17<00:00,  1.13it/s]\n",
      "100%|██████████| 10/10 [00:08<00:00,  1.16it/s]\n",
      "Train Epoch: 15 Loss: 0.5056: 100%|██████████| 88/88 [01:18<00:00,  1.12it/s]\n",
      "100%|██████████| 10/10 [00:08<00:00,  1.15it/s]\n",
      "Train Epoch: 16 Loss: 0.5312: 100%|██████████| 88/88 [01:17<00:00,  1.13it/s]\n",
      "100%|██████████| 10/10 [00:08<00:00,  1.14it/s]\n",
      "Train Epoch: 17 Loss: 0.5314: 100%|██████████| 88/88 [01:17<00:00,  1.13it/s]\n",
      "100%|██████████| 10/10 [00:08<00:00,  1.16it/s]\n",
      "Train Epoch: 18 Loss: 0.5119: 100%|██████████| 88/88 [01:18<00:00,  1.12it/s]\n",
      "100%|██████████| 10/10 [00:08<00:00,  1.16it/s]\n",
      "Train Epoch: 19 Loss: 0.544: 100%|██████████| 88/88 [01:18<00:00,  1.13it/s] \n",
      "100%|██████████| 10/10 [00:08<00:00,  1.15it/s]\n",
      "Train Epoch: 20 Loss: 0.5181: 100%|██████████| 88/88 [01:18<00:00,  1.13it/s]\n",
      "100%|██████████| 10/10 [00:08<00:00,  1.14it/s]\n",
      "Train Epoch: 21 Loss: 0.4709: 100%|██████████| 88/88 [01:17<00:00,  1.13it/s]\n",
      "100%|██████████| 10/10 [00:08<00:00,  1.15it/s]\n",
      "Train Epoch: 22 Loss: 0.537: 100%|██████████| 88/88 [01:17<00:00,  1.13it/s] \n",
      "100%|██████████| 10/10 [00:08<00:00,  1.15it/s]\n",
      "Train Epoch: 23 Loss: 0.5058: 100%|██████████| 88/88 [01:17<00:00,  1.13it/s]\n",
      "100%|██████████| 10/10 [00:08<00:00,  1.15it/s]\n",
      "Train Epoch: 24 Loss: 0.5417: 100%|██████████| 88/88 [01:18<00:00,  1.13it/s]\n",
      "100%|██████████| 10/10 [00:08<00:00,  1.16it/s]\n",
      "Train Epoch: 25 Loss: 0.4894: 100%|██████████| 88/88 [01:18<00:00,  1.13it/s]\n",
      "100%|██████████| 10/10 [00:08<00:00,  1.15it/s]\n",
      "Train Epoch: 1 Loss: 0.6374: 100%|██████████| 88/88 [01:39<00:00,  1.14s/it]\n",
      "100%|██████████| 10/10 [00:10<00:00,  1.09s/it]\n",
      "Train Epoch: 2 Loss: 0.6402: 100%|██████████| 88/88 [01:40<00:00,  1.14s/it]\n",
      "100%|██████████| 10/10 [00:10<00:00,  1.10s/it]\n",
      "Train Epoch: 3 Loss: 0.5721: 100%|██████████| 88/88 [01:39<00:00,  1.14s/it]\n",
      "100%|██████████| 10/10 [00:10<00:00,  1.10s/it]\n",
      "Train Epoch: 4 Loss: 0.5648: 100%|██████████| 88/88 [01:40<00:00,  1.14s/it]\n",
      "100%|██████████| 10/10 [00:11<00:00,  1.11s/it]\n",
      "Train Epoch: 5 Loss: 0.5959: 100%|██████████| 88/88 [01:39<00:00,  1.13s/it]\n",
      "100%|██████████| 10/10 [00:11<00:00,  1.12s/it]\n",
      "Train Epoch: 6 Loss: 0.5433: 100%|██████████| 88/88 [01:40<00:00,  1.14s/it]\n",
      "100%|██████████| 10/10 [00:11<00:00,  1.10s/it]\n",
      "Train Epoch: 7 Loss: 0.5684: 100%|██████████| 88/88 [01:40<00:00,  1.14s/it]\n",
      "100%|██████████| 10/10 [00:10<00:00,  1.09s/it]\n",
      "Train Epoch: 8 Loss: 0.5973: 100%|██████████| 88/88 [01:40<00:00,  1.14s/it]\n",
      "100%|██████████| 10/10 [00:10<00:00,  1.10s/it]\n",
      "Train Epoch: 9 Loss: 0.6622: 100%|██████████| 88/88 [01:40<00:00,  1.14s/it]\n",
      "100%|██████████| 10/10 [00:10<00:00,  1.10s/it]\n",
      "Train Epoch: 10 Loss: 0.5352: 100%|██████████| 88/88 [01:40<00:00,  1.14s/it]\n",
      "100%|██████████| 10/10 [00:10<00:00,  1.10s/it]\n",
      "Train Epoch: 11 Loss: 0.5735: 100%|██████████| 88/88 [01:39<00:00,  1.13s/it]\n",
      "100%|██████████| 10/10 [00:11<00:00,  1.11s/it]\n",
      "Train Epoch: 12 Loss: 0.5747: 100%|██████████| 88/88 [01:40<00:00,  1.14s/it]\n",
      "100%|██████████| 10/10 [00:11<00:00,  1.11s/it]\n",
      "Train Epoch: 13 Loss: 0.6149: 100%|██████████| 88/88 [01:40<00:00,  1.14s/it]\n",
      "100%|██████████| 10/10 [00:10<00:00,  1.09s/it]\n",
      "Train Epoch: 14 Loss: 0.5124: 100%|██████████| 88/88 [01:39<00:00,  1.13s/it]\n",
      "100%|██████████| 10/10 [00:11<00:00,  1.10s/it]\n",
      "Train Epoch: 15 Loss: 0.5396: 100%|██████████| 88/88 [01:40<00:00,  1.14s/it]\n",
      "100%|██████████| 10/10 [00:11<00:00,  1.11s/it]\n",
      "Train Epoch: 16 Loss: 0.5326: 100%|██████████| 88/88 [01:40<00:00,  1.14s/it]\n",
      "100%|██████████| 10/10 [00:10<00:00,  1.09s/it]\n",
      "Train Epoch: 17 Loss: 0.5592: 100%|██████████| 88/88 [01:39<00:00,  1.14s/it]\n",
      "100%|██████████| 10/10 [00:10<00:00,  1.10s/it]\n",
      "Train Epoch: 18 Loss: 0.5907: 100%|██████████| 88/88 [01:40<00:00,  1.14s/it]\n",
      "100%|██████████| 10/10 [00:10<00:00,  1.10s/it]\n",
      "Train Epoch: 19 Loss: 0.5475: 100%|██████████| 88/88 [01:40<00:00,  1.14s/it]\n",
      "100%|██████████| 10/10 [00:10<00:00,  1.10s/it]\n",
      "Downloading: \"https://download.pytorch.org/models/vgg11_bn-6002323d.pth\" to C:\\Users\\Rui/.cache\\torch\\hub\\checkpoints\\vgg11_bn-6002323d.pth\n",
      "Train Epoch: 1 Loss: 0.6105: 100%|██████████| 88/88 [01:04<00:00,  1.37it/s]\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.32it/s]\n",
      "Train Epoch: 2 Loss: 0.6329: 100%|██████████| 88/88 [01:08<00:00,  1.29it/s]\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.30it/s]\n",
      "Train Epoch: 3 Loss: 0.6256: 100%|██████████| 88/88 [01:08<00:00,  1.28it/s]\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.32it/s]\n",
      "Train Epoch: 4 Loss: 0.6132: 100%|██████████| 88/88 [01:08<00:00,  1.29it/s]\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.30it/s]\n",
      "Train Epoch: 5 Loss: 0.6431: 100%|██████████| 88/88 [01:08<00:00,  1.29it/s]\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.31it/s]\n",
      "Train Epoch: 6 Loss: 0.6261: 100%|██████████| 88/88 [01:08<00:00,  1.28it/s]\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.30it/s]\n",
      "Train Epoch: 7 Loss: 0.6518: 100%|██████████| 88/88 [01:08<00:00,  1.28it/s]\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.31it/s]\n",
      "Train Epoch: 8 Loss: 0.5773: 100%|██████████| 88/88 [01:08<00:00,  1.28it/s]\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.28it/s]\n",
      "Train Epoch: 9 Loss: 0.5945: 100%|██████████| 88/88 [01:08<00:00,  1.29it/s]\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.29it/s]\n",
      "Train Epoch: 10 Loss: 0.5438: 100%|██████████| 88/88 [01:08<00:00,  1.28it/s]\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.29it/s]\n",
      "Train Epoch: 11 Loss: 0.6351: 100%|██████████| 88/88 [01:08<00:00,  1.29it/s]\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.31it/s]\n",
      "Train Epoch: 12 Loss: 0.5264: 100%|██████████| 88/88 [01:08<00:00,  1.28it/s]\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.30it/s]\n",
      "Train Epoch: 13 Loss: 0.6838: 100%|██████████| 88/88 [01:08<00:00,  1.28it/s]\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.30it/s]\n",
      "Train Epoch: 14 Loss: 0.563: 100%|██████████| 88/88 [01:08<00:00,  1.29it/s] \n",
      "100%|██████████| 10/10 [00:07<00:00,  1.31it/s]\n",
      "Train Epoch: 15 Loss: 0.5742: 100%|██████████| 88/88 [01:08<00:00,  1.28it/s]\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.31it/s]\n",
      "Train Epoch: 16 Loss: 0.5844: 100%|██████████| 88/88 [01:08<00:00,  1.28it/s]\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.30it/s]\n",
      "Train Epoch: 17 Loss: 0.5659: 100%|██████████| 88/88 [01:08<00:00,  1.28it/s]\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.27it/s]\n",
      "Train Epoch: 18 Loss: 0.6168: 100%|██████████| 88/88 [01:08<00:00,  1.28it/s]\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.31it/s]\n",
      "Train Epoch: 19 Loss: 0.5751: 100%|██████████| 88/88 [01:09<00:00,  1.27it/s]\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.28it/s]\n",
      "Train Epoch: 20 Loss: 0.6388: 100%|██████████| 88/88 [01:08<00:00,  1.29it/s]\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.30it/s]\n",
      "Train Epoch: 21 Loss: 0.5338: 100%|██████████| 88/88 [01:08<00:00,  1.28it/s]\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.30it/s]\n",
      "Train Epoch: 22 Loss: 0.6648: 100%|██████████| 88/88 [01:09<00:00,  1.27it/s]\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.30it/s]\n",
      "Train Epoch: 23 Loss: 0.643: 100%|██████████| 88/88 [01:08<00:00,  1.28it/s] \n",
      "100%|██████████| 10/10 [00:07<00:00,  1.29it/s]\n",
      "Train Epoch: 24 Loss: 0.7145: 100%|██████████| 88/88 [01:08<00:00,  1.29it/s]\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.29it/s]\n",
      "Train Epoch: 25 Loss: 0.5692: 100%|██████████| 88/88 [01:08<00:00,  1.29it/s]\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.31it/s]\n",
      "Train Epoch: 26 Loss: 0.6164: 100%|██████████| 88/88 [01:08<00:00,  1.29it/s]\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.30it/s]\n",
      "Train Epoch: 27 Loss: 0.6445: 100%|██████████| 88/88 [01:08<00:00,  1.28it/s]\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.30it/s]\n",
      "Train Epoch: 28 Loss: 0.5824: 100%|██████████| 88/88 [01:08<00:00,  1.28it/s]\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.33it/s]\n",
      "Train Epoch: 29 Loss: 0.5799: 100%|██████████| 88/88 [01:08<00:00,  1.28it/s]\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.29it/s]\n",
      "Train Epoch: 30 Loss: 0.5864: 100%|██████████| 88/88 [01:08<00:00,  1.28it/s]\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.28it/s]\n",
      "Train Epoch: 31 Loss: 0.5996: 100%|██████████| 88/88 [01:08<00:00,  1.28it/s]\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.29it/s]\n",
      "Train Epoch: 32 Loss: 0.5739: 100%|██████████| 88/88 [01:08<00:00,  1.29it/s]\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.30it/s]\n",
      "Train Epoch: 33 Loss: 0.5507: 100%|██████████| 88/88 [01:08<00:00,  1.28it/s]\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.31it/s]\n",
      "Train Epoch: 34 Loss: 0.5671: 100%|██████████| 88/88 [01:08<00:00,  1.28it/s]\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.30it/s]\n",
      "Train Epoch: 35 Loss: 0.582: 100%|██████████| 88/88 [01:08<00:00,  1.28it/s] \n",
      "100%|██████████| 10/10 [00:07<00:00,  1.32it/s]\n",
      "Train Epoch: 36 Loss: 0.5772: 100%|██████████| 88/88 [01:08<00:00,  1.28it/s]\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.32it/s]\n",
      "Train Epoch: 37 Loss: 0.5518: 100%|██████████| 88/88 [01:08<00:00,  1.28it/s]\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.29it/s]\n",
      "Train Epoch: 38 Loss: 0.5693: 100%|██████████| 88/88 [01:08<00:00,  1.28it/s]\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.28it/s]\n",
      "Train Epoch: 39 Loss: 0.6504: 100%|██████████| 88/88 [01:09<00:00,  1.27it/s]\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.30it/s]\n",
      "Downloading: \"https://download.pytorch.org/models/vgg13_bn-abd245e5.pth\" to C:\\Users\\Rui/.cache\\torch\\hub\\checkpoints\\vgg13_bn-abd245e5.pth\n",
      "  0%|          | 0/88 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=87306240.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3f2ae6d85ceb45069e100b987daae2e3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=178728960.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4b8554b50071414091cbadf5b3bf00d9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=531503671.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b3ea547bc6c64308b930336957a6a7af"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=532246301.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5e84dd911e7849bdb0fe451891e78a32"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 6.00 GiB total capacity; 2.76 GiB already allocated; 458.44 MiB free; 3.60 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-33-e453e52f1fca>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      4\u001B[0m     \u001B[0moption\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m{\u001B[0m\u001B[1;34m\"name\"\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"arch;args;pretrained_model\"\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m     \u001B[0mlogging\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshutdown\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 6\u001B[1;33m     \u001B[0mtrainers\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrun\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"pretrained_model.json\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moption\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      7\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Documents\\deep_learning\\ocular_disease\\run_model.py\u001B[0m in \u001B[0;36mrun\u001B[1;34m(cfg_name, modification, run_id, log_model)\u001B[0m\n\u001B[0;32m     55\u001B[0m                       lr_scheduler=lr_scheduler)\n\u001B[0;32m     56\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 57\u001B[1;33m     \u001B[0mtrainer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     58\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mtrainer\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     59\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Documents\\deep_learning\\ocular_disease\\base\\base_trainer.py\u001B[0m in \u001B[0;36mtrain\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     63\u001B[0m         \u001B[0mnot_improved_count\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     64\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mepoch\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstart_epoch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mepochs\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 65\u001B[1;33m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_train_epoch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mepoch\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     66\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     67\u001B[0m             \u001B[1;31m# save logged information into log dict\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Documents\\deep_learning\\ocular_disease\\utils\\trainer.py\u001B[0m in \u001B[0;36m_train_epoch\u001B[1;34m(self, epoch)\u001B[0m\n\u001B[0;32m     56\u001B[0m                 \u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcriterion\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moutput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmu\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlog_var\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     57\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 58\u001B[1;33m                 \u001B[0moutput\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     59\u001B[0m                 \u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcriterion\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moutput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtarget\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     60\u001B[0m             \u001B[0mloss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\rui\\documents\\phd_research_rs\\ancientdiscovery\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m    887\u001B[0m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    888\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 889\u001B[1;33m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    890\u001B[0m         for hook in itertools.chain(\n\u001B[0;32m    891\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Documents\\deep_learning\\ocular_disease\\models\\pretrained_model.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     24\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     25\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 26\u001B[1;33m         \u001B[0mx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpretrained_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     27\u001B[0m         \u001B[0mx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     28\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\rui\\documents\\phd_research_rs\\ancientdiscovery\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m    887\u001B[0m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    888\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 889\u001B[1;33m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    890\u001B[0m         for hook in itertools.chain(\n\u001B[0;32m    891\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\rui\\documents\\phd_research_rs\\ancientdiscovery\\venv\\lib\\site-packages\\torchvision\\models\\vgg.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     47\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     48\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTensor\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTensor\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 49\u001B[1;33m         \u001B[0mx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     50\u001B[0m         \u001B[0mx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mavgpool\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     51\u001B[0m         \u001B[0mx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mflatten\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\rui\\documents\\phd_research_rs\\ancientdiscovery\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m    887\u001B[0m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    888\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 889\u001B[1;33m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    890\u001B[0m         for hook in itertools.chain(\n\u001B[0;32m    891\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\rui\\documents\\phd_research_rs\\ancientdiscovery\\venv\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    117\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    118\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mmodule\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 119\u001B[1;33m             \u001B[0minput\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodule\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    120\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    121\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\rui\\documents\\phd_research_rs\\ancientdiscovery\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m    887\u001B[0m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    888\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 889\u001B[1;33m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    890\u001B[0m         for hook in itertools.chain(\n\u001B[0;32m    891\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\rui\\documents\\phd_research_rs\\ancientdiscovery\\venv\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    133\u001B[0m         \u001B[1;32massert\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrunning_mean\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrunning_mean\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTensor\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    134\u001B[0m         \u001B[1;32massert\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrunning_var\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrunning_var\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTensor\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 135\u001B[1;33m         return F.batch_norm(\n\u001B[0m\u001B[0;32m    136\u001B[0m             \u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    137\u001B[0m             \u001B[1;31m# If buffers are not to be tracked, ensure that they won't be updated\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\rui\\documents\\phd_research_rs\\ancientdiscovery\\venv\\lib\\site-packages\\torch\\nn\\functional.py\u001B[0m in \u001B[0;36mbatch_norm\u001B[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001B[0m\n\u001B[0;32m   2147\u001B[0m         \u001B[0m_verify_batch_size\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2148\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2149\u001B[1;33m     return torch.batch_norm(\n\u001B[0m\u001B[0;32m   2150\u001B[0m         \u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mweight\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbias\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrunning_mean\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrunning_var\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtraining\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmomentum\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0meps\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackends\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcudnn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0menabled\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2151\u001B[0m     )\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 6.00 GiB total capacity; 2.76 GiB already allocated; 458.44 MiB free; 3.60 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "trainers = []\n",
    "for model in all_models[:5]:\n",
    "    print(f\"Run model: {model}\")\n",
    "    option.update({\"name\": model, \"arch;args;pretrained_model\": model})\n",
    "    logging.shutdown()\n",
    "    trainers.append(run(\"pretrained_model.json\", option, model))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for trainer in trainers:\n",
    "    trainer.model.cpu()\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run model: vgg13_bn\n",
      "Run model: vgg16_bn\n",
      "\n",
      "Run model: vgg19_bn\n",
      "Run model: densenet121\n",
      "\n",
      "Run model: densenet169\n",
      "\n",
      "Run model: densenet201\n",
      "\n",
      "Run model: densenet161\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 Loss: 0.6038: 100%|██████████| 176/176 [01:18<00:00,  2.24it/s]\n",
      "100%|██████████| 20/20 [00:09<00:00,  2.22it/s]\n",
      "Train Epoch: 2 Loss: 0.6038: 100%|██████████| 176/176 [01:22<00:00,  2.12it/s]\n",
      "100%|██████████| 20/20 [00:09<00:00,  2.12it/s]\n",
      "Train Epoch: 3 Loss: 0.5829: 100%|██████████| 176/176 [01:26<00:00,  2.04it/s]\n",
      "100%|██████████| 20/20 [00:09<00:00,  2.07it/s]\n",
      "Train Epoch: 4 Loss: 0.5812: 100%|██████████| 176/176 [01:27<00:00,  2.02it/s]\n",
      "100%|██████████| 20/20 [00:09<00:00,  2.07it/s]\n",
      "Train Epoch: 5 Loss: 0.5962: 100%|██████████| 176/176 [01:28<00:00,  2.00it/s]\n",
      "100%|██████████| 20/20 [00:09<00:00,  2.04it/s]\n",
      "Train Epoch: 6 Loss: 0.5786: 100%|██████████| 176/176 [01:28<00:00,  1.99it/s]\n",
      "100%|██████████| 20/20 [00:09<00:00,  2.04it/s]\n",
      "Train Epoch: 7 Loss: 0.5579: 100%|██████████| 176/176 [01:28<00:00,  1.99it/s]\n",
      "100%|██████████| 20/20 [00:09<00:00,  2.05it/s]\n",
      "Train Epoch: 8 Loss: 0.6459: 100%|██████████| 176/176 [01:28<00:00,  1.98it/s]\n",
      "100%|██████████| 20/20 [00:09<00:00,  2.07it/s]\n",
      "Train Epoch: 9 Loss: 0.5875: 100%|██████████| 176/176 [01:28<00:00,  1.98it/s]\n",
      "100%|██████████| 20/20 [00:09<00:00,  2.05it/s]\n",
      "Train Epoch: 10 Loss: 0.6425: 100%|██████████| 176/176 [01:29<00:00,  1.98it/s]\n",
      "100%|██████████| 20/20 [00:09<00:00,  2.02it/s]\n",
      "Train Epoch: 11 Loss: 0.5209: 100%|██████████| 176/176 [01:29<00:00,  1.98it/s]\n",
      "100%|██████████| 20/20 [00:09<00:00,  2.01it/s]\n",
      "Train Epoch: 12 Loss: 0.6069: 100%|██████████| 176/176 [01:29<00:00,  1.98it/s]\n",
      "100%|██████████| 20/20 [00:09<00:00,  2.02it/s]\n",
      "Train Epoch: 13 Loss: 0.5385: 100%|██████████| 176/176 [01:29<00:00,  1.97it/s]\n",
      "100%|██████████| 20/20 [00:09<00:00,  2.02it/s]\n",
      "Train Epoch: 14 Loss: 0.5462: 100%|██████████| 176/176 [01:31<00:00,  1.92it/s]\n",
      "100%|██████████| 20/20 [00:10<00:00,  1.88it/s]\n",
      "Train Epoch: 15 Loss: 0.5369: 100%|██████████| 176/176 [01:35<00:00,  1.84it/s]\n",
      "100%|██████████| 20/20 [00:10<00:00,  1.87it/s]\n",
      "Train Epoch: 16 Loss: 0.6444: 100%|██████████| 176/176 [01:36<00:00,  1.83it/s]\n",
      "100%|██████████| 20/20 [00:10<00:00,  1.86it/s]\n",
      "Train Epoch: 17 Loss: 0.6299: 100%|██████████| 176/176 [01:36<00:00,  1.82it/s]\n",
      "100%|██████████| 20/20 [00:10<00:00,  1.87it/s]\n",
      "Train Epoch: 18 Loss: 0.5198: 100%|██████████| 176/176 [01:35<00:00,  1.83it/s]\n",
      "100%|██████████| 20/20 [00:10<00:00,  1.86it/s]\n",
      "Train Epoch: 19 Loss: 0.5564: 100%|██████████| 176/176 [01:36<00:00,  1.83it/s]\n",
      "100%|██████████| 20/20 [00:10<00:00,  1.87it/s]\n",
      "Train Epoch: 20 Loss: 0.5688: 100%|██████████| 176/176 [01:36<00:00,  1.83it/s]\n",
      "100%|██████████| 20/20 [00:10<00:00,  1.86it/s]\n",
      "Train Epoch: 21 Loss: 0.5691: 100%|██████████| 176/176 [01:36<00:00,  1.83it/s]\n",
      "100%|██████████| 20/20 [00:10<00:00,  1.85it/s]\n",
      "Train Epoch: 22 Loss: 0.5209: 100%|██████████| 176/176 [01:36<00:00,  1.82it/s]\n",
      "100%|██████████| 20/20 [00:10<00:00,  1.87it/s]\n",
      "Downloading: \"https://download.pytorch.org/models/vgg16_bn-6c64b313.pth\" to C:\\Users\\Rui/.cache\\torch\\hub\\checkpoints\\vgg16_bn-6c64b313.pth\n",
      "Train Epoch: 1 Loss: 0.7011: 100%|██████████| 176/176 [01:42<00:00,  1.72it/s]\n",
      "100%|██████████| 20/20 [00:11<00:00,  1.69it/s]\n",
      "Train Epoch: 2 Loss: 0.6354: 100%|██████████| 176/176 [01:47<00:00,  1.64it/s]\n",
      "100%|██████████| 20/20 [00:11<00:00,  1.68it/s]\n",
      "Train Epoch: 3 Loss: 0.6621: 100%|██████████| 176/176 [01:47<00:00,  1.64it/s]\n",
      "100%|██████████| 20/20 [00:11<00:00,  1.68it/s]\n",
      "Train Epoch: 4 Loss: 0.5689: 100%|██████████| 176/176 [01:47<00:00,  1.64it/s]\n",
      "100%|██████████| 20/20 [00:11<00:00,  1.67it/s]\n",
      "Train Epoch: 5 Loss: 0.6319: 100%|██████████| 176/176 [01:47<00:00,  1.64it/s]\n",
      "100%|██████████| 20/20 [00:11<00:00,  1.67it/s]\n",
      "Train Epoch: 6 Loss: 0.6335: 100%|██████████| 176/176 [01:47<00:00,  1.64it/s]\n",
      "100%|██████████| 20/20 [00:11<00:00,  1.68it/s]\n",
      "Train Epoch: 7 Loss: 0.5672: 100%|██████████| 176/176 [01:46<00:00,  1.65it/s]\n",
      "100%|██████████| 20/20 [00:11<00:00,  1.69it/s]\n",
      "Train Epoch: 8 Loss: 0.5316: 100%|██████████| 176/176 [01:47<00:00,  1.63it/s]\n",
      "100%|██████████| 20/20 [00:12<00:00,  1.54it/s]\n",
      "Train Epoch: 9 Loss: 0.5447: 100%|██████████| 176/176 [01:51<00:00,  1.59it/s]\n",
      "100%|██████████| 20/20 [00:12<00:00,  1.66it/s]\n",
      "Train Epoch: 10 Loss: 0.5296: 100%|██████████| 176/176 [01:49<00:00,  1.61it/s]\n",
      "100%|██████████| 20/20 [00:12<00:00,  1.66it/s]\n",
      "Train Epoch: 11 Loss: 0.6057: 100%|██████████| 176/176 [01:49<00:00,  1.61it/s]\n",
      "100%|██████████| 20/20 [00:12<00:00,  1.61it/s]\n",
      "Train Epoch: 12 Loss: 0.57: 100%|██████████| 176/176 [01:48<00:00,  1.62it/s]  \n",
      "100%|██████████| 20/20 [00:12<00:00,  1.65it/s]\n",
      "Train Epoch: 13 Loss: 0.5206: 100%|██████████| 176/176 [01:49<00:00,  1.60it/s]\n",
      "100%|██████████| 20/20 [00:12<00:00,  1.57it/s]\n",
      "Train Epoch: 14 Loss: 0.6637: 100%|██████████| 176/176 [01:52<00:00,  1.56it/s]\n",
      "100%|██████████| 20/20 [00:12<00:00,  1.55it/s]\n",
      "Train Epoch: 15 Loss: 0.6038: 100%|██████████| 176/176 [01:56<00:00,  1.51it/s]\n",
      "100%|██████████| 20/20 [00:12<00:00,  1.62it/s]\n",
      "Train Epoch: 16 Loss: 0.5724: 100%|██████████| 176/176 [01:52<00:00,  1.57it/s]\n",
      "100%|██████████| 20/20 [00:12<00:00,  1.54it/s]\n",
      "Train Epoch: 17 Loss: 0.5406: 100%|██████████| 176/176 [01:50<00:00,  1.59it/s]\n",
      "100%|██████████| 20/20 [00:12<00:00,  1.61it/s]\n",
      "Train Epoch: 18 Loss: 0.5704: 100%|██████████| 176/176 [01:49<00:00,  1.60it/s]\n",
      "100%|██████████| 20/20 [00:12<00:00,  1.62it/s]\n",
      "Train Epoch: 19 Loss: 0.7194: 100%|██████████| 176/176 [01:52<00:00,  1.56it/s]\n",
      "100%|██████████| 20/20 [00:12<00:00,  1.60it/s]\n",
      "Train Epoch: 1 Loss: 0.642: 100%|██████████| 176/176 [02:08<00:00,  1.37it/s] \n",
      "100%|██████████| 20/20 [00:14<00:00,  1.40it/s]\n",
      "Train Epoch: 2 Loss: 0.7245: 100%|██████████| 176/176 [02:08<00:00,  1.37it/s]\n",
      "100%|██████████| 20/20 [00:14<00:00,  1.38it/s]\n",
      "Train Epoch: 3 Loss: 0.5953: 100%|██████████| 176/176 [02:07<00:00,  1.38it/s]\n",
      "100%|██████████| 20/20 [00:14<00:00,  1.40it/s]\n",
      "Train Epoch: 4 Loss: 0.5798: 100%|██████████| 176/176 [02:09<00:00,  1.36it/s]\n",
      "100%|██████████| 20/20 [00:14<00:00,  1.35it/s]\n",
      "Train Epoch: 5 Loss: 0.5726: 100%|██████████| 176/176 [02:14<00:00,  1.31it/s]\n",
      "100%|██████████| 20/20 [00:15<00:00,  1.33it/s]\n",
      "Train Epoch: 6 Loss: 0.6667: 100%|██████████| 176/176 [02:14<00:00,  1.31it/s]\n",
      "100%|██████████| 20/20 [00:14<00:00,  1.39it/s]\n",
      "Train Epoch: 7 Loss: 0.4944: 100%|██████████| 176/176 [02:10<00:00,  1.35it/s]\n",
      "100%|██████████| 20/20 [00:14<00:00,  1.39it/s]\n",
      "Train Epoch: 8 Loss: 0.5557: 100%|██████████| 176/176 [02:09<00:00,  1.36it/s]\n",
      "100%|██████████| 20/20 [00:14<00:00,  1.40it/s]\n",
      "Train Epoch: 9 Loss: 0.6627: 100%|██████████| 176/176 [02:08<00:00,  1.37it/s]\n",
      "100%|██████████| 20/20 [00:14<00:00,  1.38it/s]\n",
      "Train Epoch: 10 Loss: 0.5354: 100%|██████████| 176/176 [02:07<00:00,  1.38it/s]\n",
      "100%|██████████| 20/20 [00:14<00:00,  1.38it/s]\n",
      "Train Epoch: 11 Loss: 0.6043: 100%|██████████| 176/176 [02:10<00:00,  1.35it/s]\n",
      "100%|██████████| 20/20 [00:14<00:00,  1.35it/s]\n",
      "Train Epoch: 12 Loss: 0.5955: 100%|██████████| 176/176 [02:14<00:00,  1.31it/s]\n",
      "100%|██████████| 20/20 [00:14<00:00,  1.34it/s]\n",
      "Train Epoch: 13 Loss: 0.5527: 100%|██████████| 176/176 [02:14<00:00,  1.31it/s]\n",
      "100%|██████████| 20/20 [00:14<00:00,  1.33it/s]\n",
      "Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to C:\\Users\\Rui/.cache\\torch\\hub\\checkpoints\\densenet121-a639ec97.pth\n",
      "Train Epoch: 1 Loss: 0.6482: 100%|██████████| 176/176 [01:13<00:00,  2.39it/s]\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.40it/s]\n",
      "Train Epoch: 2 Loss: 0.6261: 100%|██████████| 176/176 [01:13<00:00,  2.39it/s]\n",
      "100%|██████████| 20/20 [00:07<00:00,  2.68it/s]\n",
      "Train Epoch: 3 Loss: 0.6957: 100%|██████████| 176/176 [01:11<00:00,  2.45it/s]\n",
      "100%|██████████| 20/20 [00:07<00:00,  2.52it/s]\n",
      "Train Epoch: 4 Loss: 0.5949: 100%|██████████| 176/176 [01:11<00:00,  2.46it/s]\n",
      "100%|██████████| 20/20 [00:07<00:00,  2.51it/s]\n",
      "Train Epoch: 5 Loss: 0.5296: 100%|██████████| 176/176 [01:11<00:00,  2.48it/s]\n",
      "100%|██████████| 20/20 [00:07<00:00,  2.56it/s]\n",
      "Train Epoch: 6 Loss: 0.5323: 100%|██████████| 176/176 [01:10<00:00,  2.49it/s]\n",
      "100%|██████████| 20/20 [00:07<00:00,  2.57it/s]\n",
      "Train Epoch: 7 Loss: 0.5553: 100%|██████████| 176/176 [01:11<00:00,  2.45it/s]\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.45it/s]\n",
      "Train Epoch: 8 Loss: 0.6998: 100%|██████████| 176/176 [01:15<00:00,  2.34it/s]\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.35it/s]\n",
      "Train Epoch: 9 Loss: 0.5964: 100%|██████████| 176/176 [01:12<00:00,  2.43it/s]\n",
      "100%|██████████| 20/20 [00:07<00:00,  2.52it/s]\n",
      "Train Epoch: 10 Loss: 0.4399: 100%|██████████| 176/176 [01:13<00:00,  2.39it/s]\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.36it/s]\n",
      "Train Epoch: 11 Loss: 0.5268: 100%|██████████| 176/176 [01:12<00:00,  2.44it/s]\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.46it/s]\n",
      "Train Epoch: 12 Loss: 0.6087: 100%|██████████| 176/176 [01:11<00:00,  2.46it/s]\n",
      "100%|██████████| 20/20 [00:07<00:00,  2.53it/s]\n",
      "Train Epoch: 13 Loss: 0.5176: 100%|██████████| 176/176 [01:10<00:00,  2.48it/s]\n",
      "100%|██████████| 20/20 [00:07<00:00,  2.53it/s]\n",
      "Train Epoch: 14 Loss: 0.5342: 100%|██████████| 176/176 [01:11<00:00,  2.46it/s]\n",
      "100%|██████████| 20/20 [00:07<00:00,  2.53it/s]\n",
      "Train Epoch: 15 Loss: 0.565: 100%|██████████| 176/176 [01:12<00:00,  2.43it/s] \n",
      "100%|██████████| 20/20 [00:08<00:00,  2.48it/s]\n",
      "Downloading: \"https://download.pytorch.org/models/densenet169-b2777c0a.pth\" to C:\\Users\\Rui/.cache\\torch\\hub\\checkpoints\\densenet169-b2777c0a.pth\n",
      "Train Epoch: 1 Loss: 0.654: 100%|██████████| 176/176 [01:18<00:00,  2.24it/s] \n",
      "100%|██████████| 20/20 [00:08<00:00,  2.26it/s]\n",
      "Train Epoch: 2 Loss: 0.5362: 100%|██████████| 176/176 [01:19<00:00,  2.22it/s]\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.31it/s]\n",
      "Train Epoch: 3 Loss: 0.6207: 100%|██████████| 176/176 [01:19<00:00,  2.22it/s]\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.30it/s]\n",
      "Train Epoch: 4 Loss: 0.5417: 100%|██████████| 176/176 [01:17<00:00,  2.26it/s]\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.32it/s]\n",
      "Train Epoch: 5 Loss: 0.5129: 100%|██████████| 176/176 [01:19<00:00,  2.23it/s]\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.31it/s]\n",
      "Train Epoch: 6 Loss: 0.5182: 100%|██████████| 176/176 [01:19<00:00,  2.22it/s]\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.28it/s]\n",
      "Train Epoch: 7 Loss: 0.5219: 100%|██████████| 176/176 [01:18<00:00,  2.23it/s]\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.31it/s]\n",
      "Train Epoch: 8 Loss: 0.5201: 100%|██████████| 176/176 [01:18<00:00,  2.25it/s]\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.31it/s]\n",
      "Train Epoch: 9 Loss: 0.5196: 100%|██████████| 176/176 [01:17<00:00,  2.26it/s]\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.30it/s]\n",
      "Train Epoch: 10 Loss: 0.5779: 100%|██████████| 176/176 [01:18<00:00,  2.23it/s]\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.30it/s]\n",
      "Train Epoch: 11 Loss: 0.5152: 100%|██████████| 176/176 [01:19<00:00,  2.22it/s]\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.35it/s]\n",
      "Train Epoch: 12 Loss: 0.5608: 100%|██████████| 176/176 [01:18<00:00,  2.24it/s]\n",
      "100%|██████████| 20/20 [00:09<00:00,  2.20it/s]\n",
      "Train Epoch: 13 Loss: 0.5295: 100%|██████████| 176/176 [01:19<00:00,  2.21it/s]\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.33it/s]\n",
      "Train Epoch: 14 Loss: 0.5548: 100%|██████████| 176/176 [01:18<00:00,  2.25it/s]\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.30it/s]\n",
      "Train Epoch: 15 Loss: 0.5358: 100%|██████████| 176/176 [01:18<00:00,  2.23it/s]\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.27it/s]\n",
      "Train Epoch: 16 Loss: 0.5691: 100%|██████████| 176/176 [01:18<00:00,  2.24it/s]\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.27it/s]\n",
      "Train Epoch: 17 Loss: 0.5013: 100%|██████████| 176/176 [01:18<00:00,  2.25it/s]\n",
      "100%|██████████| 20/20 [00:09<00:00,  2.19it/s]\n",
      "Train Epoch: 18 Loss: 0.5098: 100%|██████████| 176/176 [01:19<00:00,  2.22it/s]\n",
      "100%|██████████| 20/20 [00:09<00:00,  2.22it/s]\n",
      "Train Epoch: 19 Loss: 0.5846: 100%|██████████| 176/176 [01:16<00:00,  2.29it/s]\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.46it/s]\n",
      "Train Epoch: 20 Loss: 0.453: 100%|██████████| 176/176 [01:13<00:00,  2.39it/s] \n",
      "100%|██████████| 20/20 [00:08<00:00,  2.45it/s]\n",
      "Train Epoch: 21 Loss: 0.5235: 100%|██████████| 176/176 [01:14<00:00,  2.35it/s]\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.46it/s]\n",
      "Train Epoch: 22 Loss: 0.4737: 100%|██████████| 176/176 [01:13<00:00,  2.41it/s]\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.44it/s]\n",
      "Train Epoch: 23 Loss: 0.4402: 100%|██████████| 176/176 [01:14<00:00,  2.36it/s]\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.31it/s]\n",
      "Downloading: \"https://download.pytorch.org/models/densenet201-c1103571.pth\" to C:\\Users\\Rui/.cache\\torch\\hub\\checkpoints\\densenet201-c1103571.pth\n",
      "Train Epoch: 1 Loss: 0.5696: 100%|██████████| 176/176 [01:23<00:00,  2.10it/s]\n",
      "100%|██████████| 20/20 [00:09<00:00,  2.08it/s]\n",
      "Train Epoch: 2 Loss: 0.5631: 100%|██████████| 176/176 [01:27<00:00,  2.02it/s]\n",
      "100%|██████████| 20/20 [00:09<00:00,  2.01it/s]\n",
      "Train Epoch: 3 Loss: 0.5799: 100%|██████████| 176/176 [01:29<00:00,  1.97it/s]\n",
      "100%|██████████| 20/20 [00:09<00:00,  2.06it/s]\n",
      "Train Epoch: 4 Loss: 0.6139: 100%|██████████| 176/176 [01:28<00:00,  1.99it/s]\n",
      "100%|██████████| 20/20 [00:09<00:00,  2.04it/s]\n",
      "Train Epoch: 5 Loss: 0.5764: 100%|██████████| 176/176 [01:29<00:00,  1.97it/s]\n",
      "100%|██████████| 20/20 [00:09<00:00,  2.06it/s]\n",
      "Train Epoch: 6 Loss: 0.538: 100%|██████████| 176/176 [01:25<00:00,  2.06it/s] \n",
      "100%|██████████| 20/20 [00:09<00:00,  2.13it/s]\n",
      "Train Epoch: 7 Loss: 0.6244: 100%|██████████| 176/176 [01:24<00:00,  2.07it/s]\n",
      "100%|██████████| 20/20 [00:09<00:00,  2.21it/s]\n",
      "Train Epoch: 8 Loss: 0.5349: 100%|██████████| 176/176 [01:24<00:00,  2.09it/s]\n",
      "100%|██████████| 20/20 [00:09<00:00,  2.04it/s]\n",
      "Train Epoch: 9 Loss: 0.4638: 100%|██████████| 176/176 [01:29<00:00,  1.96it/s]\n",
      "100%|██████████| 20/20 [00:09<00:00,  2.16it/s]\n",
      "Train Epoch: 10 Loss: 0.4841: 100%|██████████| 176/176 [01:26<00:00,  2.03it/s]\n",
      "100%|██████████| 20/20 [00:09<00:00,  2.05it/s]\n",
      "Train Epoch: 11 Loss: 0.5837: 100%|██████████| 176/176 [01:29<00:00,  1.97it/s]\n",
      "100%|██████████| 20/20 [00:09<00:00,  2.05it/s]\n",
      "Downloading: \"https://download.pytorch.org/models/densenet161-8d451a50.pth\" to C:\\Users\\Rui/.cache\\torch\\hub\\checkpoints\\densenet161-8d451a50.pth\n",
      "Train Epoch: 1 Loss: 0.5918: 100%|██████████| 176/176 [01:48<00:00,  1.62it/s]\n",
      "100%|██████████| 20/20 [00:12<00:00,  1.60it/s]\n",
      "Train Epoch: 2 Loss: 0.6191: 100%|██████████| 176/176 [01:51<00:00,  1.58it/s]\n",
      "100%|██████████| 20/20 [00:12<00:00,  1.66it/s]\n",
      "Train Epoch: 3 Loss: 0.5373: 100%|██████████| 176/176 [01:49<00:00,  1.61it/s]\n",
      "100%|██████████| 20/20 [00:11<00:00,  1.69it/s]\n",
      "Train Epoch: 4 Loss: 0.5397: 100%|██████████| 176/176 [01:48<00:00,  1.62it/s]\n",
      "100%|██████████| 20/20 [00:11<00:00,  1.69it/s]\n",
      "Train Epoch: 5 Loss: 0.5713: 100%|██████████| 176/176 [01:47<00:00,  1.63it/s]\n",
      "100%|██████████| 20/20 [00:11<00:00,  1.76it/s]\n",
      "Train Epoch: 6 Loss: 0.5987: 100%|██████████| 176/176 [01:42<00:00,  1.72it/s]\n",
      "100%|██████████| 20/20 [00:11<00:00,  1.80it/s]\n",
      "Train Epoch: 7 Loss: 0.6408: 100%|██████████| 176/176 [01:41<00:00,  1.74it/s]\n",
      "100%|██████████| 20/20 [00:11<00:00,  1.80it/s]\n",
      "Train Epoch: 8 Loss: 0.4701: 100%|██████████| 176/176 [01:41<00:00,  1.73it/s]\n",
      "100%|██████████| 20/20 [00:11<00:00,  1.81it/s]\n",
      "Train Epoch: 9 Loss: 0.5656: 100%|██████████| 176/176 [01:40<00:00,  1.74it/s]\n",
      "100%|██████████| 20/20 [00:11<00:00,  1.80it/s]\n",
      "Train Epoch: 10 Loss: 0.5345: 100%|██████████| 176/176 [01:43<00:00,  1.70it/s]\n",
      "100%|██████████| 20/20 [00:11<00:00,  1.77it/s]\n",
      "Train Epoch: 11 Loss: 0.5372: 100%|██████████| 176/176 [01:42<00:00,  1.71it/s]\n",
      "100%|██████████| 20/20 [00:11<00:00,  1.77it/s]\n",
      "Train Epoch: 12 Loss: 0.5457: 100%|██████████| 176/176 [01:42<00:00,  1.72it/s]\n",
      "100%|██████████| 20/20 [00:11<00:00,  1.72it/s]\n",
      "Train Epoch: 13 Loss: 0.5825: 100%|██████████| 176/176 [01:44<00:00,  1.69it/s]\n",
      "100%|██████████| 20/20 [00:11<00:00,  1.70it/s]\n",
      "Train Epoch: 14 Loss: 0.5292: 100%|██████████| 176/176 [01:43<00:00,  1.70it/s]\n",
      "100%|██████████| 20/20 [00:11<00:00,  1.76it/s]\n",
      "Train Epoch: 15 Loss: 0.4522: 100%|██████████| 176/176 [01:43<00:00,  1.70it/s]\n",
      "100%|██████████| 20/20 [00:11<00:00,  1.74it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=553507836.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c2b39420fb184f89a74d18b90541fd50"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=32342954.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5a28f5a1e09f461c83fffb4b85ef7fd4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=57365526.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "40ddba305a554987a911515bb2db5200"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=81131730.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f142fd9c2d824792826bf8c484b3feb7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=115730790.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6b0812ed87af4f8e96d4d5e2d9cfa1e9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "option = {\n",
    "    \"arch;args;pretrained_model\": \"googlenet\", # pretrained model name\n",
    "    \"arch;args;freeze_param\": 1, # freeze the parameters of pretrained model\n",
    "    \"data_loader;args;img_size\": 256,\n",
    "    \"data_loader;args;batch_size\": 32,\n",
    "    \"optimizer;args;lr\": 0.0001\n",
    "}\n",
    "trainers = []\n",
    "for model in all_models[5:]:\n",
    "    print(f\"Run model: {model}\")\n",
    "    option.update({\"name\": model, \"arch;args;pretrained_model\": model})\n",
    "    logging.shutdown()\n",
    "    trainers.append(run(\"pretrained_model.json\", option, model))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run model: resnet101\n",
      "Run model: vgg11_bn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 Loss: 0.6217: 100%|██████████| 176/176 [04:14<00:00,  1.44s/it]\n",
      "100%|██████████| 20/20 [00:28<00:00,  1.41s/it]\n",
      "Train Epoch: 2 Loss: 0.6512: 100%|██████████| 176/176 [04:19<00:00,  1.48s/it]\n",
      "100%|██████████| 20/20 [00:28<00:00,  1.42s/it]\n",
      "Train Epoch: 3 Loss: 0.6753: 100%|██████████| 176/176 [04:20<00:00,  1.48s/it]\n",
      "100%|██████████| 20/20 [00:28<00:00,  1.43s/it]\n",
      "Train Epoch: 4 Loss: 0.5883: 100%|██████████| 176/176 [04:14<00:00,  1.45s/it]\n",
      "100%|██████████| 20/20 [00:26<00:00,  1.35s/it]\n",
      "Train Epoch: 5 Loss: 0.6097: 100%|██████████| 176/176 [04:03<00:00,  1.39s/it]\n",
      "100%|██████████| 20/20 [00:26<00:00,  1.33s/it]\n",
      "Train Epoch: 6 Loss: 0.6203: 100%|██████████| 176/176 [04:03<00:00,  1.38s/it]\n",
      "100%|██████████| 20/20 [00:26<00:00,  1.34s/it]\n",
      "Train Epoch: 7 Loss: 0.5759: 100%|██████████| 176/176 [04:22<00:00,  1.49s/it]\n",
      "100%|██████████| 20/20 [00:28<00:00,  1.42s/it]\n",
      "Train Epoch: 8 Loss: 0.5898: 100%|██████████| 176/176 [04:16<00:00,  1.46s/it]\n",
      "100%|██████████| 20/20 [00:28<00:00,  1.43s/it]\n",
      "Train Epoch: 9 Loss: 0.6314: 100%|██████████| 176/176 [04:19<00:00,  1.48s/it]\n",
      "100%|██████████| 20/20 [00:28<00:00,  1.41s/it]\n",
      "Train Epoch: 10 Loss: 0.5967: 100%|██████████| 176/176 [04:21<00:00,  1.49s/it]\n",
      "100%|██████████| 20/20 [00:29<00:00,  1.46s/it]\n",
      "Train Epoch: 11 Loss: 0.6051: 100%|██████████| 176/176 [04:20<00:00,  1.48s/it]\n",
      "100%|██████████| 20/20 [00:28<00:00,  1.43s/it]\n",
      "Train Epoch: 12 Loss: 0.6599: 100%|██████████| 176/176 [04:20<00:00,  1.48s/it]\n",
      "100%|██████████| 20/20 [00:28<00:00,  1.43s/it]\n",
      "Train Epoch: 13 Loss: 0.5923: 100%|██████████| 176/176 [04:19<00:00,  1.48s/it]\n",
      "100%|██████████| 20/20 [00:28<00:00,  1.43s/it]\n",
      "Train Epoch: 14 Loss: 0.5839: 100%|██████████| 176/176 [04:19<00:00,  1.47s/it]\n",
      "100%|██████████| 20/20 [00:28<00:00,  1.41s/it]\n",
      "Train Epoch: 15 Loss: 0.6212: 100%|██████████| 176/176 [04:18<00:00,  1.47s/it]\n",
      "100%|██████████| 20/20 [00:28<00:00,  1.42s/it]\n",
      "Train Epoch: 16 Loss: 0.5398: 100%|██████████| 176/176 [04:16<00:00,  1.46s/it]\n",
      "100%|██████████| 20/20 [00:28<00:00,  1.41s/it]\n",
      "Train Epoch: 17 Loss: 0.5466: 100%|██████████| 176/176 [04:18<00:00,  1.47s/it]\n",
      "100%|██████████| 20/20 [00:28<00:00,  1.43s/it]\n",
      "Train Epoch: 18 Loss: 0.5322: 100%|██████████| 176/176 [04:19<00:00,  1.48s/it]\n",
      "100%|██████████| 20/20 [00:28<00:00,  1.40s/it]\n",
      "Train Epoch: 19 Loss: 0.6289: 100%|██████████| 176/176 [04:15<00:00,  1.45s/it]\n",
      "100%|██████████| 20/20 [00:28<00:00,  1.41s/it]\n",
      "Train Epoch: 20 Loss: 0.5121: 100%|██████████| 176/176 [04:15<00:00,  1.45s/it]\n",
      "100%|██████████| 20/20 [00:27<00:00,  1.39s/it]\n",
      "Train Epoch: 21 Loss: 0.5702: 100%|██████████| 176/176 [04:14<00:00,  1.44s/it]\n",
      "100%|██████████| 20/20 [00:27<00:00,  1.40s/it]\n",
      "Train Epoch: 22 Loss: 0.5717: 100%|██████████| 176/176 [04:15<00:00,  1.45s/it]\n",
      "100%|██████████| 20/20 [00:29<00:00,  1.48s/it]\n",
      "Train Epoch: 23 Loss: 0.5055: 100%|██████████| 176/176 [04:22<00:00,  1.49s/it]\n",
      "100%|██████████| 20/20 [00:28<00:00,  1.43s/it]\n",
      "Train Epoch: 24 Loss: 0.5695: 100%|██████████| 176/176 [04:23<00:00,  1.50s/it]\n",
      "100%|██████████| 20/20 [00:28<00:00,  1.44s/it]\n",
      "Train Epoch: 25 Loss: 0.5437: 100%|██████████| 176/176 [04:19<00:00,  1.48s/it]\n",
      "100%|██████████| 20/20 [00:28<00:00,  1.42s/it]\n",
      "Train Epoch: 26 Loss: 0.6108: 100%|██████████| 176/176 [04:24<00:00,  1.50s/it]\n",
      "100%|██████████| 20/20 [00:28<00:00,  1.44s/it]\n",
      "Train Epoch: 27 Loss: 0.5749: 100%|██████████| 176/176 [04:22<00:00,  1.49s/it]\n",
      "100%|██████████| 20/20 [00:28<00:00,  1.43s/it]\n",
      "Train Epoch: 28 Loss: 0.5879: 100%|██████████| 176/176 [04:18<00:00,  1.47s/it]\n",
      "100%|██████████| 20/20 [00:27<00:00,  1.39s/it]\n",
      "Train Epoch: 29 Loss: 0.5581: 100%|██████████| 176/176 [04:15<00:00,  1.45s/it]\n",
      "100%|██████████| 20/20 [00:27<00:00,  1.39s/it]\n",
      "Train Epoch: 30 Loss: 0.4838: 100%|██████████| 176/176 [04:14<00:00,  1.44s/it]\n",
      "100%|██████████| 20/20 [00:28<00:00,  1.40s/it]\n",
      "Train Epoch: 31 Loss: 0.4397: 100%|██████████| 176/176 [04:14<00:00,  1.44s/it]\n",
      "100%|██████████| 20/20 [00:27<00:00,  1.39s/it]\n",
      "Train Epoch: 32 Loss: 0.4943: 100%|██████████| 176/176 [04:14<00:00,  1.45s/it]\n",
      "100%|██████████| 20/20 [00:27<00:00,  1.38s/it]\n",
      "Train Epoch: 33 Loss: 0.509: 100%|██████████| 176/176 [04:13<00:00,  1.44s/it] \n",
      "100%|██████████| 20/20 [00:27<00:00,  1.39s/it]\n",
      "Train Epoch: 34 Loss: 0.5016: 100%|██████████| 176/176 [04:14<00:00,  1.44s/it]\n",
      "100%|██████████| 20/20 [00:28<00:00,  1.42s/it]\n",
      "Train Epoch: 35 Loss: 0.6808: 100%|██████████| 176/176 [04:17<00:00,  1.46s/it]\n",
      "100%|██████████| 20/20 [00:28<00:00,  1.41s/it]\n",
      "Train Epoch: 36 Loss: 0.517: 100%|██████████| 176/176 [04:19<00:00,  1.47s/it] \n",
      "100%|██████████| 20/20 [00:28<00:00,  1.40s/it]\n",
      "Train Epoch: 37 Loss: 0.5774: 100%|██████████| 176/176 [04:17<00:00,  1.46s/it]\n",
      "100%|██████████| 20/20 [00:27<00:00,  1.40s/it]\n",
      "Train Epoch: 38 Loss: 0.5007: 100%|██████████| 176/176 [04:15<00:00,  1.45s/it]\n",
      "100%|██████████| 20/20 [00:28<00:00,  1.40s/it]\n",
      "Train Epoch: 39 Loss: 0.5489: 100%|██████████| 176/176 [04:18<00:00,  1.47s/it]\n",
      "100%|██████████| 20/20 [00:28<00:00,  1.42s/it]\n",
      "Train Epoch: 40 Loss: 0.5889: 100%|██████████| 176/176 [04:20<00:00,  1.48s/it]\n",
      "100%|██████████| 20/20 [00:28<00:00,  1.43s/it]\n",
      "Train Epoch: 41 Loss: 0.4909: 100%|██████████| 176/176 [04:20<00:00,  1.48s/it]\n",
      "100%|██████████| 20/20 [00:28<00:00,  1.43s/it]\n",
      "Train Epoch: 42 Loss: 0.5549: 100%|██████████| 176/176 [04:20<00:00,  1.48s/it]\n",
      "100%|██████████| 20/20 [00:28<00:00,  1.44s/it]\n",
      "Train Epoch: 43 Loss: 0.5166: 100%|██████████| 176/176 [04:15<00:00,  1.45s/it]\n",
      "100%|██████████| 20/20 [00:26<00:00,  1.35s/it]\n",
      "Train Epoch: 44 Loss: 0.512: 100%|██████████| 176/176 [04:03<00:00,  1.38s/it] \n",
      "100%|██████████| 20/20 [00:26<00:00,  1.33s/it]\n",
      "  0%|          | 0/176 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 GiB (GPU 0; 6.00 GiB total capacity; 2.76 GiB already allocated; 1.27 GiB free; 2.79 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-2-d1c21f23e278>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     13\u001B[0m     \u001B[0moption\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m{\u001B[0m\u001B[1;34m\"name\"\u001B[0m\u001B[1;33m:\u001B[0m \u001B[1;34m\"best_model\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"arch;args;pretrained_model\"\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     14\u001B[0m     \u001B[0mlogging\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshutdown\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 15\u001B[1;33m     \u001B[0mtrainers_512\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrun\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"pretrained_model.json\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moption\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34mf\"{model}\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     16\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Documents\\deep_learning\\ocular_disease\\run_model.py\u001B[0m in \u001B[0;36mrun\u001B[1;34m(cfg_name, modification, run_id, log_model)\u001B[0m\n\u001B[0;32m     55\u001B[0m                       lr_scheduler=lr_scheduler)\n\u001B[0;32m     56\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 57\u001B[1;33m     \u001B[0mtrainer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     58\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mtrainer\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     59\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Documents\\deep_learning\\ocular_disease\\base\\base_trainer.py\u001B[0m in \u001B[0;36mtrain\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     63\u001B[0m         \u001B[0mnot_improved_count\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     64\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mepoch\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstart_epoch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mepochs\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 65\u001B[1;33m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_train_epoch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mepoch\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     66\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     67\u001B[0m             \u001B[1;31m# save logged information into log dict\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Documents\\deep_learning\\ocular_disease\\utils\\trainer.py\u001B[0m in \u001B[0;36m_train_epoch\u001B[1;34m(self, epoch)\u001B[0m\n\u001B[0;32m     56\u001B[0m                 \u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcriterion\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moutput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmu\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlog_var\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     57\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 58\u001B[1;33m                 \u001B[0moutput\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     59\u001B[0m                 \u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcriterion\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moutput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtarget\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     60\u001B[0m             \u001B[0mloss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\rui\\documents\\phd_research_rs\\ancientdiscovery\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m    887\u001B[0m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    888\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 889\u001B[1;33m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    890\u001B[0m         for hook in itertools.chain(\n\u001B[0;32m    891\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Documents\\deep_learning\\ocular_disease\\models\\pretrained_model.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     25\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     26\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 27\u001B[1;33m         \u001B[0mx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpretrained_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     28\u001B[0m         \u001B[0mx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     29\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\rui\\documents\\phd_research_rs\\ancientdiscovery\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m    887\u001B[0m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    888\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 889\u001B[1;33m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    890\u001B[0m         for hook in itertools.chain(\n\u001B[0;32m    891\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\rui\\documents\\phd_research_rs\\ancientdiscovery\\venv\\lib\\site-packages\\torchvision\\models\\vgg.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     47\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     48\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTensor\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTensor\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 49\u001B[1;33m         \u001B[0mx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     50\u001B[0m         \u001B[0mx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mavgpool\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     51\u001B[0m         \u001B[0mx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mflatten\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\rui\\documents\\phd_research_rs\\ancientdiscovery\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m    887\u001B[0m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    888\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 889\u001B[1;33m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    890\u001B[0m         for hook in itertools.chain(\n\u001B[0;32m    891\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\rui\\documents\\phd_research_rs\\ancientdiscovery\\venv\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    117\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    118\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mmodule\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 119\u001B[1;33m             \u001B[0minput\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodule\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    120\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    121\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\rui\\documents\\phd_research_rs\\ancientdiscovery\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m    887\u001B[0m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    888\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 889\u001B[1;33m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    890\u001B[0m         for hook in itertools.chain(\n\u001B[0;32m    891\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\rui\\documents\\phd_research_rs\\ancientdiscovery\\venv\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    133\u001B[0m         \u001B[1;32massert\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrunning_mean\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrunning_mean\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTensor\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    134\u001B[0m         \u001B[1;32massert\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrunning_var\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrunning_var\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTensor\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 135\u001B[1;33m         return F.batch_norm(\n\u001B[0m\u001B[0;32m    136\u001B[0m             \u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    137\u001B[0m             \u001B[1;31m# If buffers are not to be tracked, ensure that they won't be updated\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\rui\\documents\\phd_research_rs\\ancientdiscovery\\venv\\lib\\site-packages\\torch\\nn\\functional.py\u001B[0m in \u001B[0;36mbatch_norm\u001B[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001B[0m\n\u001B[0;32m   2147\u001B[0m         \u001B[0m_verify_batch_size\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2148\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2149\u001B[1;33m     return torch.batch_norm(\n\u001B[0m\u001B[0;32m   2150\u001B[0m         \u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mweight\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbias\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrunning_mean\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrunning_var\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtraining\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmomentum\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0meps\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackends\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcudnn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0menabled\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2151\u001B[0m     )\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA out of memory. Tried to allocate 2.00 GiB (GPU 0; 6.00 GiB total capacity; 2.76 GiB already allocated; 1.27 GiB free; 2.79 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "# run with larger image size, the batch size can be too large\n",
    "option = {\n",
    "    \"arch;args;pretrained_model\": \"googlenet\", # pretrained model name\n",
    "    \"arch;args;freeze_param\": 1, # freeze the parameters of pretrained model\n",
    "    \"data_loader;args;img_size\": 512,\n",
    "    \"data_loader;args;batch_size\": 32,\n",
    "    \"optimizer;args;lr\": 0.0001\n",
    "}\n",
    "# run with dropout=0.2\n",
    "trainers_512 = []\n",
    "for model in [\"resnet101\", \"vgg11_bn\", \"densenet169\"]:\n",
    "    print(f\"Run model: {model}\")\n",
    "    option.update({\"name\": \"best_model\", \"arch;args;pretrained_model\": model})\n",
    "    logging.shutdown()\n",
    "    trainers_512.append(run(\"pretrained_model.json\", option, f\"{model}\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "\n",
    "def run_test(conf, test_model):\n",
    "    # setup data_loader instances\n",
    "    data_loader = conf.init_obj(\"data_loader\", module_data)\n",
    "\n",
    "    # prepare for (multi-device) GPU training\n",
    "    device, device_ids = prepare_device(conf[\"n_gpu\"])\n",
    "    test_model = test_model.to(device)\n",
    "    if len(device_ids) > 1:\n",
    "        test_model = torch.nn.DataParallel(test_model, device_ids=device_ids)\n",
    "\n",
    "    # get function handles of loss and metrics\n",
    "    criterion = getattr(module_loss, conf[\"loss\"])\n",
    "    metrics = [getattr(module_metric, met) for met in conf[\"metrics\"]]\n",
    "\n",
    "    # build optimizer, learning rate scheduler. delete every lines containing lr_scheduler for disabling scheduler\n",
    "    trainable_params = filter(lambda p: p.requires_grad, test_model.parameters())\n",
    "    optimizer = conf.init_obj(\"optimizer\", torch.optim, trainable_params)\n",
    "    lr_scheduler = conf.init_obj(\"lr_scheduler\", torch.optim.lr_scheduler, optimizer)\n",
    "\n",
    "    # define trainer of model\n",
    "    test_trainer = Trainer(test_model, criterion, metrics, optimizer,\n",
    "                      config=conf,\n",
    "                      device=device,\n",
    "                      data_loader=data_loader.train_loader,\n",
    "                      valid_data_loader=data_loader.valid_loader,\n",
    "                      lr_scheduler=lr_scheduler)\n",
    "    return test_trainer._valid_epoch(0)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:03<00:00,  6.03it/s]\n",
      "100%|██████████| 40/40 [00:04<00:00,  8.86it/s]\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.51it/s]\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.31it/s]\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.26it/s]\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.30it/s]\n",
      "100%|██████████| 20/20 [00:09<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_bn() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torchvision.models.googlenet.BasicConv2d'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torch.nn.modules.container.Sequential'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torchvision.models.googlenet.Inception'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torchvision.models.googlenet.GoogLeNet'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torch.nn.modules.activation.Softmax'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'models.pretrained_model.PretrainedModel'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_bn() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torchvision.models.googlenet.BasicConv2d'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torch.nn.modules.container.Sequential'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torchvision.models.googlenet.Inception'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torchvision.models.googlenet.GoogLeNet'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torch.nn.modules.activation.Softmax'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'models.pretrained_model.PretrainedModel'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_bn() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torchvision.models.googlenet.BasicConv2d'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torch.nn.modules.container.Sequential'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torchvision.models.googlenet.Inception'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torchvision.models.googlenet.GoogLeNet'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torch.nn.modules.activation.Softmax'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'models.pretrained_model.PretrainedModel'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_bn() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torchvision.models.googlenet.BasicConv2d'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torch.nn.modules.container.Sequential'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torchvision.models.googlenet.Inception'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torchvision.models.googlenet.GoogLeNet'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torch.nn.modules.activation.Softmax'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'models.pretrained_model.PretrainedModel'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_bn() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torchvision.models.googlenet.BasicConv2d'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torch.nn.modules.container.Sequential'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torchvision.models.googlenet.Inception'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torchvision.models.googlenet.GoogLeNet'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torch.nn.modules.activation.Softmax'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'models.pretrained_model.PretrainedModel'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_bn() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torchvision.models.googlenet.BasicConv2d'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torch.nn.modules.container.Sequential'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torchvision.models.googlenet.Inception'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torchvision.models.googlenet.GoogLeNet'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torch.nn.modules.activation.Softmax'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'models.pretrained_model.PretrainedModel'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_bn() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torchvision.models.googlenet.BasicConv2d'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torch.nn.modules.container.Sequential'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torchvision.models.googlenet.Inception'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torchvision.models.googlenet.GoogLeNet'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torch.nn.modules.activation.Softmax'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'models.pretrained_model.PretrainedModel'>. Treat it as zero Macs and zero Params.\u001B[00m\n"
     ]
    }
   ],
   "source": [
    "evaluation_df = pd.DataFrame(columns=[\"model\", \"val_auc\", \"val_acc\", \"test_auc\", \"test_acc\", \"hyper_parameters\", \"parameters\", \"FLOPS\", \"Time Consuming\"])\n",
    "# load the statistic information\n",
    "for path in os.scandir(\"saved/models/googlenet\"):\n",
    "    hyper_parameters = path.name\n",
    "    ckpt_path = os.path.join(path, \"model_best.pth\")\n",
    "    checkpoint = torch.load(ckpt_path, map_location=torch.device('cuda'))\n",
    "    log_dic = checkpoint[\"log\"]\n",
    "    checkpoint[\"config\"][\"name\"] = \"test\"\n",
    "    config = ConfigParser(checkpoint[\"config\"], modification={\"data_loader;args;valid_df_path\": \"dataset/test_df.csv\"})\n",
    "    model = config.init_obj(\"arch\", module_arch)\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    start_time = time.time()\n",
    "    test_log = run_test(config, model)\n",
    "    time_consume = round(time.time() - start_time, 2)\n",
    "\n",
    "    img_size, bs, lr = path.name.split(\"_\")\n",
    "    flops, params = profile(model, inputs=(torch.randn(int(bs), 3, int(img_size), int(img_size)).to(torch.device(\"cuda\")), ))\n",
    "    flops, params = clever_format([flops, params], \"%.3f\")\n",
    "    line = pd.Series({\n",
    "    \"model\": \"GoogLeNet\", \"val_auc\": log_dic[\"val_auc\"], \"val_acc\": log_dic[\"val_accuracy\"],\n",
    "        \"hyper_parameters\": path.name, \"parameters\": params, \"FLOPS\": flops,\n",
    "        \"test_auc\": test_log[\"auc\"], \"test_acc\": test_log[\"accuracy\"], \"Time Consuming\": f\"{time_consume} s\"\n",
    "    })\n",
    "    evaluation_df = evaluation_df.append(line, ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "       model  val_auc  val_acc  test_auc  test_acc hyper_parameters  \\\n0  GoogLeNet   0.6749   0.6322    0.6714    0.6338     128_32_0.001   \n1  GoogLeNet   0.7315   0.6573    0.7588    0.7011     256_16_0.001   \n2  GoogLeNet   0.7291   0.6651    0.7314    0.6729     256_32_0.001   \n3  GoogLeNet   0.7500   0.7027    0.7499    0.6823    256_64_0.0001   \n4  GoogLeNet   0.7363   0.6761    0.7356    0.6729     256_64_0.001   \n5  GoogLeNet   0.6447   0.5947    0.6294    0.5900      256_64_0.01   \n6  GoogLeNet   0.7373   0.6541    0.7417    0.6714     512_32_0.001   \n\n  parameters     FLOPS Time Consuming  \n0     6.753M   15.751G          3.4 s  \n1     6.753M   31.446G         4.58 s  \n2     6.753M   62.892G          4.5 s  \n3     6.753M  125.784G         4.45 s  \n4     6.753M  125.784G         4.51 s  \n5     6.753M  125.784G         4.43 s  \n6     6.753M  251.457G         9.91 s  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>val_auc</th>\n      <th>val_acc</th>\n      <th>test_auc</th>\n      <th>test_acc</th>\n      <th>hyper_parameters</th>\n      <th>parameters</th>\n      <th>FLOPS</th>\n      <th>Time Consuming</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>GoogLeNet</td>\n      <td>0.6749</td>\n      <td>0.6322</td>\n      <td>0.6714</td>\n      <td>0.6338</td>\n      <td>128_32_0.001</td>\n      <td>6.753M</td>\n      <td>15.751G</td>\n      <td>3.4 s</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>GoogLeNet</td>\n      <td>0.7315</td>\n      <td>0.6573</td>\n      <td>0.7588</td>\n      <td>0.7011</td>\n      <td>256_16_0.001</td>\n      <td>6.753M</td>\n      <td>31.446G</td>\n      <td>4.58 s</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>GoogLeNet</td>\n      <td>0.7291</td>\n      <td>0.6651</td>\n      <td>0.7314</td>\n      <td>0.6729</td>\n      <td>256_32_0.001</td>\n      <td>6.753M</td>\n      <td>62.892G</td>\n      <td>4.5 s</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>GoogLeNet</td>\n      <td>0.7500</td>\n      <td>0.7027</td>\n      <td>0.7499</td>\n      <td>0.6823</td>\n      <td>256_64_0.0001</td>\n      <td>6.753M</td>\n      <td>125.784G</td>\n      <td>4.45 s</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>GoogLeNet</td>\n      <td>0.7363</td>\n      <td>0.6761</td>\n      <td>0.7356</td>\n      <td>0.6729</td>\n      <td>256_64_0.001</td>\n      <td>6.753M</td>\n      <td>125.784G</td>\n      <td>4.51 s</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>GoogLeNet</td>\n      <td>0.6447</td>\n      <td>0.5947</td>\n      <td>0.6294</td>\n      <td>0.5900</td>\n      <td>256_64_0.01</td>\n      <td>6.753M</td>\n      <td>125.784G</td>\n      <td>4.43 s</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>GoogLeNet</td>\n      <td>0.7373</td>\n      <td>0.6541</td>\n      <td>0.7417</td>\n      <td>0.6714</td>\n      <td>512_32_0.001</td>\n      <td>6.753M</td>\n      <td>251.457G</td>\n      <td>9.91 s</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_df.to_csv(os.path.join(result_path, \"hyper_tuning.csv\"), index=False)\n",
    "evaluation_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:05<00:00,  1.89it/s]\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.83it/s]\n",
      "100%|██████████| 10/10 [00:06<00:00,  1.46it/s]\n",
      "100%|██████████| 10/10 [00:08<00:00,  1.18it/s]\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.79it/s]\n",
      "100%|██████████| 20/20 [00:07<00:00,  2.80it/s]\n",
      "100%|██████████| 20/20 [00:07<00:00,  2.60it/s]\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.43it/s]\n",
      "100%|██████████| 20/20 [00:06<00:00,  3.25it/s]\n",
      "100%|██████████| 20/20 [00:06<00:00,  3.01it/s]\n",
      "100%|██████████| 20/20 [00:07<00:00,  2.57it/s]\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_bn() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torchvision.models.resnet.BasicBlock'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torch.nn.modules.container.Sequential'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torchvision.models.resnet.ResNet'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torch.nn.modules.activation.Softmax'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'models.pretrained_model.PretrainedModel'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_bn() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torch.nn.modules.container.Sequential'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torchvision.models.resnet.Bottleneck'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torchvision.models.resnet.ResNet'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torch.nn.modules.activation.Softmax'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'models.pretrained_model.PretrainedModel'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_bn() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torch.nn.modules.container.Sequential'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torchvision.models.resnet.Bottleneck'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torchvision.models.resnet.ResNet'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torch.nn.modules.activation.Softmax'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'models.pretrained_model.PretrainedModel'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_bn() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torch.nn.modules.container.Sequential'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torchvision.models.resnet.Bottleneck'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torchvision.models.resnet.ResNet'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torch.nn.modules.activation.Softmax'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'models.pretrained_model.PretrainedModel'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_bn() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torch.nn.modules.container.Sequential'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torchvision.models.vgg.VGG'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torch.nn.modules.activation.Softmax'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'models.pretrained_model.PretrainedModel'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_bn() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torch.nn.modules.container.Sequential'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torchvision.models.vgg.VGG'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torch.nn.modules.activation.Softmax'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'models.pretrained_model.PretrainedModel'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_bn() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torch.nn.modules.container.Sequential'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torchvision.models.vgg.VGG'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torch.nn.modules.activation.Softmax'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'models.pretrained_model.PretrainedModel'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_bn() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torch.nn.modules.container.Sequential'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torchvision.models.vgg.VGG'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torch.nn.modules.activation.Softmax'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'models.pretrained_model.PretrainedModel'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_bn() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torchvision.models.densenet._DenseLayer'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torchvision.models.densenet._DenseBlock'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "[INFO] Register count_avgpool() for <class 'torch.nn.modules.pooling.AvgPool2d'>.\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torchvision.models.densenet._Transition'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torch.nn.modules.container.Sequential'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torchvision.models.densenet.DenseNet'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torch.nn.modules.activation.Softmax'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'models.pretrained_model.PretrainedModel'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_bn() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torchvision.models.densenet._DenseLayer'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torchvision.models.densenet._DenseBlock'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "[INFO] Register count_avgpool() for <class 'torch.nn.modules.pooling.AvgPool2d'>.\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torchvision.models.densenet._Transition'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torch.nn.modules.container.Sequential'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torchvision.models.densenet.DenseNet'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torch.nn.modules.activation.Softmax'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'models.pretrained_model.PretrainedModel'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_bn() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torchvision.models.densenet._DenseLayer'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torchvision.models.densenet._DenseBlock'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "[INFO] Register count_avgpool() for <class 'torch.nn.modules.pooling.AvgPool2d'>.\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torchvision.models.densenet._Transition'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torch.nn.modules.container.Sequential'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torchvision.models.densenet.DenseNet'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torch.nn.modules.activation.Softmax'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'models.pretrained_model.PretrainedModel'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_bn() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torchvision.models.densenet._DenseLayer'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torchvision.models.densenet._DenseBlock'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "[INFO] Register count_avgpool() for <class 'torch.nn.modules.pooling.AvgPool2d'>.\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torchvision.models.densenet._Transition'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torch.nn.modules.container.Sequential'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torchvision.models.densenet.DenseNet'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'torch.nn.modules.activation.Softmax'>. Treat it as zero Macs and zero Params.\u001B[00m\n",
      "\u001B[91m[WARN] Cannot find rule for <class 'models.pretrained_model.PretrainedModel'>. Treat it as zero Macs and zero Params.\u001B[00m\n"
     ]
    }
   ],
   "source": [
    "pretrained_compare_df = pd.DataFrame(columns=[\"model\", \"val_auc\", \"val_acc\", \"test_auc\", \"test_acc\", \"parameters\", \"FLOPS\", \"Time Consuming\"])\n",
    "for model_name in all_models:\n",
    "    log_dir = os.path.join(\"saved/models\", model_name)\n",
    "    ckpt_path = os.path.join(log_dir, os.listdir(log_dir)[-1], \"model_best.pth\")\n",
    "    checkpoint = torch.load(ckpt_path, map_location=torch.device('cpu'))\n",
    "    log_dic = checkpoint[\"log\"]\n",
    "\n",
    "    checkpoint[\"config\"][\"name\"] = \"test\"\n",
    "    config = ConfigParser(checkpoint[\"config\"], modification={\"data_loader;args;valid_df_path\": \"dataset/test_df.csv\"})\n",
    "    model = config.init_obj(\"arch\", module_arch)\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    start_time = time.time()\n",
    "    test_log = run_test(config, model)\n",
    "    time_consume = round(time.time() - start_time, 2)\n",
    "\n",
    "    img_size, bs = 256, 32\n",
    "    model = model.cpu()\n",
    "    flops, params = profile(model, inputs=(torch.randn(int(bs), 3, int(img_size), int(img_size)), ))\n",
    "    flops, params = clever_format([flops, params], \"%.3f\")\n",
    "    line = pd.Series({\n",
    "    \"model\": model_name, \"val_auc\": log_dic[\"val_auc\"], \"val_acc\": log_dic[\"val_accuracy\"],\n",
    "        \"parameters\": params, \"FLOPS\": flops, \"test_auc\": test_log[\"auc\"], \"test_acc\": test_log[\"accuracy\"],\n",
    "        \"Time Consuming\": f\"{time_consume}s\"\n",
    "    })\n",
    "    torch.cuda.empty_cache()\n",
    "    pretrained_compare_df = pretrained_compare_df.append(line, ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "          model  val_auc  val_acc  test_auc  test_acc parameters     FLOPS  \\\n0      resnet34   0.7330   0.6635    0.7795    0.7089    21.926M  153.443G   \n1      resnet50   0.7165   0.6541    0.7639    0.6854    25.685M  171.829G   \n2     resnet101   0.7441   0.6557    0.7645    0.6948    44.678M  327.412G   \n3     resnet152   0.7357   0.6573    0.7619    0.7027    60.321M  483.096G   \n4      vgg11_bn   0.7389   0.6635    0.7554    0.6839   132.997M  317.755G   \n5      vgg13_bn   0.7280   0.6510    0.7524    0.6776   133.182M  472.977G   \n6      vgg16_bn   0.7265   0.6588    0.7732    0.7089   138.494M  647.087G   \n7      vgg19_bn   0.7179   0.6322    0.7520    0.6792   143.807M  821.197G   \n8   densenet121   0.7419   0.6604    0.7647    0.6870     8.107M  119.767G   \n9   densenet169   0.7683   0.6980    0.7760    0.7074    14.278M  142.013G   \n10  densenet201   0.7372   0.6620    0.7616    0.7042    20.142M  181.420G   \n11  densenet161   0.7326   0.6526    0.7814    0.7042    28.809M  325.448G   \n\n   Time Consuming  \n0           5.42s  \n1           5.59s  \n2           7.02s  \n3           8.71s  \n4           5.85s  \n5            7.4s  \n6            8.0s  \n7           8.49s  \n8           6.26s  \n9            6.8s  \n10          7.93s  \n11          9.07s  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>val_auc</th>\n      <th>val_acc</th>\n      <th>test_auc</th>\n      <th>test_acc</th>\n      <th>parameters</th>\n      <th>FLOPS</th>\n      <th>Time Consuming</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>resnet34</td>\n      <td>0.7330</td>\n      <td>0.6635</td>\n      <td>0.7795</td>\n      <td>0.7089</td>\n      <td>21.926M</td>\n      <td>153.443G</td>\n      <td>5.42s</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>resnet50</td>\n      <td>0.7165</td>\n      <td>0.6541</td>\n      <td>0.7639</td>\n      <td>0.6854</td>\n      <td>25.685M</td>\n      <td>171.829G</td>\n      <td>5.59s</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>resnet101</td>\n      <td>0.7441</td>\n      <td>0.6557</td>\n      <td>0.7645</td>\n      <td>0.6948</td>\n      <td>44.678M</td>\n      <td>327.412G</td>\n      <td>7.02s</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>resnet152</td>\n      <td>0.7357</td>\n      <td>0.6573</td>\n      <td>0.7619</td>\n      <td>0.7027</td>\n      <td>60.321M</td>\n      <td>483.096G</td>\n      <td>8.71s</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>vgg11_bn</td>\n      <td>0.7389</td>\n      <td>0.6635</td>\n      <td>0.7554</td>\n      <td>0.6839</td>\n      <td>132.997M</td>\n      <td>317.755G</td>\n      <td>5.85s</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>vgg13_bn</td>\n      <td>0.7280</td>\n      <td>0.6510</td>\n      <td>0.7524</td>\n      <td>0.6776</td>\n      <td>133.182M</td>\n      <td>472.977G</td>\n      <td>7.4s</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>vgg16_bn</td>\n      <td>0.7265</td>\n      <td>0.6588</td>\n      <td>0.7732</td>\n      <td>0.7089</td>\n      <td>138.494M</td>\n      <td>647.087G</td>\n      <td>8.0s</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>vgg19_bn</td>\n      <td>0.7179</td>\n      <td>0.6322</td>\n      <td>0.7520</td>\n      <td>0.6792</td>\n      <td>143.807M</td>\n      <td>821.197G</td>\n      <td>8.49s</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>densenet121</td>\n      <td>0.7419</td>\n      <td>0.6604</td>\n      <td>0.7647</td>\n      <td>0.6870</td>\n      <td>8.107M</td>\n      <td>119.767G</td>\n      <td>6.26s</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>densenet169</td>\n      <td>0.7683</td>\n      <td>0.6980</td>\n      <td>0.7760</td>\n      <td>0.7074</td>\n      <td>14.278M</td>\n      <td>142.013G</td>\n      <td>6.8s</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>densenet201</td>\n      <td>0.7372</td>\n      <td>0.6620</td>\n      <td>0.7616</td>\n      <td>0.7042</td>\n      <td>20.142M</td>\n      <td>181.420G</td>\n      <td>7.93s</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>densenet161</td>\n      <td>0.7326</td>\n      <td>0.6526</td>\n      <td>0.7814</td>\n      <td>0.7042</td>\n      <td>28.809M</td>\n      <td>325.448G</td>\n      <td>9.07s</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_compare_df.to_csv(os.path.join(result_path, \"pretrained_model_compare.csv\"), index=False)\n",
    "pretrained_compare_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}