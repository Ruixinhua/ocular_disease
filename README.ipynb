{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Project - Ocular Disease Recognition\n",
    "The following is a detailed README file that allows the user to re-run data pre-processing, training, validation and evaluation.\n",
    "I did some hyper-parameter tuning and implement several models to recognize whether the color fundus images from left and right eyes is normal or disease.\n",
    "The best hyper-parameter was selected for each model, and you can reproduce the searching process and get exactly the same result as I show you."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Install requirements"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that as I use the pytorch framework for the project, and if you need to use GPU to accelerate the process,\n",
    "you maybe need to check the corresponding torch version for your CUDA driver in the website: https://pytorch.org/get-started/previous-versions/.\n",
    "For example, I used torch 1.8.1+cu111 and 0.9.1+cu111 for this project, so I can install the torch and torchvision by the following command:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "! pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 -f https://download.pytorch.org/whl/torch_stable.html"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data pre-processing\n",
    "\n",
    "Before modelling data, I did some pre-processing work on the original ODIR-5K dataset.\n",
    "As the author of ODIR-5K dataset provided a preprocessing_images folder, I will use that folder for training and validation.\n",
    "The preprocessing_images folder only contains 6392 images, compare to the original \"Training Images\" folder which contains 7000 images,\n",
    "it removes some low quality images, crop and resize all images to 512*512. For details about dataset analysis, check jupyter notebook [dataset_analysis](dataset_analysis.ipynb)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# run data pre-process on Testing Images by default with crop tasks\n",
    "! python data_preprocess.py"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The flag used in data_preprocess script includes:\n",
    "- -s: source path, by default is \"dataset/ODIR-5K/ODIR-5K/Testing Images\"\n",
    "- -d: destination path, by default is \"dataset/test_crop\"\n",
    "- -w: width of image after resizing, by default is 512, get 512*512 square images.\n",
    "- -t: specify the task from \"crop\" and \"resize\"\n",
    "- -q: quality of images, by default is 100\n",
    "- -k: whether keep aspect ratio, by default is false.\n",
    "\n",
    "You can check the pre-process [log](saved/log/data_process.log) under directory saved/log, with filename is data_process.log"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# resize and flip test images\n",
    "! python data_preprocess.py -s dataset/test_crop -d dataset/test_512 -t resize"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# resize and flip  train images\n",
    "! python data_preprocess.py -s dataset/preprocessed_images -d dataset/train_512 -w 512 -t resize"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dataset split\n",
    "The preprocessed image's dataset are divided into train, valid, and test set according to the number of each class with a pre-defined ratio.\n",
    "I tried to find the labels of \"ODIR-5K Testing Images\", but I did not find it, so I can not use it to evaluate my models.\n",
    "That is why I split another test set for this task.\n",
    "Run the dataset_split.py script, it will return the csv file of each partition.\n",
    "The flag used in the dataset_split.py includes:\n",
    "\n",
    "- -f: the path of full_df.csv, default is \"dataset/full_df.csv\"\n",
    "- -v: the ratio of validation set, default is 0.15\n",
    "- -t: the ratio of test set, default is 0.15\n",
    "- -s: the saved directory of produced csv file(train_df.csv, valid_df.csv, test_df.csv), default is \"dataset/\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set info:\n",
      " Count: 5114, positive label(N): 2299, negative label(not N): 2815\n",
      "Validation set info:\n",
      " Count: 639, positive label(N): 287, negative label(not N): 352\n",
      "Test set info:\n",
      " Count: 639, positive label(N): 287, negative label(not N): 352\n"
     ]
    }
   ],
   "source": [
    "# run dataset_split.py script\n",
    "! python dataset_split.py -v 0.1 -t 0.1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This will generate three files under saved directory(-s): train_df.csv, valid_df.csv, test_df.csv\n",
    "#### Data augmentation\n",
    "The Training set is not balanced, the number of negative label(2463) is higher than the number of positive label(2011).\n",
    "So we need to take some measures to balance the set. The most popular strategies include down-sampling and up-sampling.\n",
    "Therefore, some image augmentation strategies are applied to argument dataset through script data_argument.py with flags:\n",
    "- -t: the path to train_df.csv, default is dataset/train_df.csv\n",
    "- -s: the path to new saved train_df_aug.csv, default is dataset/train_df_aug.csv\n",
    "- -r: the path to root dataset, default is dataset/train_512\n",
    "- -a: the path to augment dataset, default is dataset/train_aug"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "! python data_augment.py"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Experiment\n",
    "As the main part of the dataset is image data, I consider to start with a series of CNN models with pretrained weight in torchvision models library.\n",
    "There are many CNN models pre-defined in pytorch, and you can find a list [here](https://pytorch.org/vision/stable/models.html#classification).\n",
    "For these pretrained model, I defined a configuration file named [pretrained_model.json](pretrained_model.json).\n",
    "For evaluation methods, I recorded loss of training and validation sets and calculated AUC(area under curve) and Accuracy for comparison.\n",
    "\n",
    "#### Pretrained model fine-tuning\n",
    "The most popular CNN model family is ResNet, VGG, and DenseNet, so for the rest of experiments, I will focus on them.\n",
    "And they all have some variant models, it will take hours to train these models even with GPU resources.\n",
    "Therefore, I firstly start with a relative small model named GoogLeNet to find the best hyper-parameter that is not tightly related with specific model.\n",
    "Such as image size, batch size and learning rate, in order to save time, I will not use grid search.\n",
    "Instead, each time when a group of hyper-parameters is tested, the best one will apply to the next group directly."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "       model  val_auc  val_acc  test_auc  test_acc hyper_parameters  \\\n0  GoogLeNet   0.6749   0.6322    0.6714    0.6338     128_32_0.001   \n1  GoogLeNet   0.7315   0.6573    0.7588    0.7011     256_16_0.001   \n2  GoogLeNet   0.7291   0.6651    0.7314    0.6729     256_32_0.001   \n3  GoogLeNet   0.7500   0.7027    0.7499    0.6823    256_64_0.0001   \n4  GoogLeNet   0.7363   0.6761    0.7356    0.6729     256_64_0.001   \n5  GoogLeNet   0.6447   0.5947    0.6294    0.5900      256_64_0.01   \n6  GoogLeNet   0.7373   0.6541    0.7417    0.6714     512_32_0.001   \n\n  parameters     FLOPS Time Consuming  \n0     6.753M   15.751G          3.4 s  \n1     6.753M   31.446G         4.58 s  \n2     6.753M   62.892G          4.5 s  \n3     6.753M  125.784G         4.45 s  \n4     6.753M  125.784G         4.51 s  \n5     6.753M  125.784G         4.43 s  \n6     6.753M  251.457G         9.91 s  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>val_auc</th>\n      <th>val_acc</th>\n      <th>test_auc</th>\n      <th>test_acc</th>\n      <th>hyper_parameters</th>\n      <th>parameters</th>\n      <th>FLOPS</th>\n      <th>Time Consuming</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>GoogLeNet</td>\n      <td>0.6749</td>\n      <td>0.6322</td>\n      <td>0.6714</td>\n      <td>0.6338</td>\n      <td>128_32_0.001</td>\n      <td>6.753M</td>\n      <td>15.751G</td>\n      <td>3.4 s</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>GoogLeNet</td>\n      <td>0.7315</td>\n      <td>0.6573</td>\n      <td>0.7588</td>\n      <td>0.7011</td>\n      <td>256_16_0.001</td>\n      <td>6.753M</td>\n      <td>31.446G</td>\n      <td>4.58 s</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>GoogLeNet</td>\n      <td>0.7291</td>\n      <td>0.6651</td>\n      <td>0.7314</td>\n      <td>0.6729</td>\n      <td>256_32_0.001</td>\n      <td>6.753M</td>\n      <td>62.892G</td>\n      <td>4.5 s</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>GoogLeNet</td>\n      <td>0.7500</td>\n      <td>0.7027</td>\n      <td>0.7499</td>\n      <td>0.6823</td>\n      <td>256_64_0.0001</td>\n      <td>6.753M</td>\n      <td>125.784G</td>\n      <td>4.45 s</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>GoogLeNet</td>\n      <td>0.7363</td>\n      <td>0.6761</td>\n      <td>0.7356</td>\n      <td>0.6729</td>\n      <td>256_64_0.001</td>\n      <td>6.753M</td>\n      <td>125.784G</td>\n      <td>4.51 s</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>GoogLeNet</td>\n      <td>0.6447</td>\n      <td>0.5947</td>\n      <td>0.6294</td>\n      <td>0.5900</td>\n      <td>256_64_0.01</td>\n      <td>6.753M</td>\n      <td>125.784G</td>\n      <td>4.43 s</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>GoogLeNet</td>\n      <td>0.7373</td>\n      <td>0.6541</td>\n      <td>0.7417</td>\n      <td>0.6714</td>\n      <td>512_32_0.001</td>\n      <td>6.753M</td>\n      <td>251.457G</td>\n      <td>9.91 s</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# I have included the result csv file, it can reproduced in experiment notebook\n",
    "hyper_tuning_df = pd.read_csv(\"saved/result/hyper_tuning.csv\")\n",
    "hyper_tuning_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrllll}\n",
      "\\toprule\n",
      "    model &  val\\_auc &  val\\_acc &  test\\_auc &  test\\_acc & hyper\\_parameters & parameters &    FLOPS & Time Consuming \\\\\n",
      "\\midrule\n",
      "GoogLeNet &   0.6749 &   0.6322 &    0.6714 &    0.6338 &     128\\_32\\_0.001 &     6.753M &  15.751G &          3.4 s \\\\\n",
      "GoogLeNet &   0.7315 &   0.6573 &    0.7588 &    0.7011 &     256\\_16\\_0.001 &     6.753M &  31.446G &         4.58 s \\\\\n",
      "GoogLeNet &   0.7291 &   0.6651 &    0.7314 &    0.6729 &     256\\_32\\_0.001 &     6.753M &  62.892G &          4.5 s \\\\\n",
      "GoogLeNet &   0.7500 &   0.7027 &    0.7499 &    0.6823 &    256\\_64\\_0.0001 &     6.753M & 125.784G &         4.45 s \\\\\n",
      "GoogLeNet &   0.7363 &   0.6761 &    0.7356 &    0.6729 &     256\\_64\\_0.001 &     6.753M & 125.784G &         4.51 s \\\\\n",
      "GoogLeNet &   0.6447 &   0.5947 &    0.6294 &    0.5900 &      256\\_64\\_0.01 &     6.753M & 125.784G &         4.43 s \\\\\n",
      "GoogLeNet &   0.7373 &   0.6541 &    0.7417 &    0.6714 &     512\\_32\\_0.001 &     6.753M & 251.457G &         9.91 s \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(hyper_tuning_df.to_latex(index=False))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As presented in the above table, the accuracy and AUC of the model increase with the increase of the image size,\n",
    "while the running speed (measured by FLOPS) gradually slows down.\n",
    "The memory usage of the model is directly proportional to the batch size and image size,\n",
    "and the batch size has a relatively small impact on the performance of the model.\n",
    "Therefore, when the GPU memory is insufficient, the problem is mainly solved by reducing the batch size.\n",
    "The setup of learning rate affects the performance a lot, and a relatively small learning rate can improve the performance of models."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "          model  val_auc  val_acc  test_auc  test_acc parameters     FLOPS  \\\n0      resnet34   0.7330   0.6635    0.7795    0.7089    21.926M  153.443G   \n1      resnet50   0.7165   0.6541    0.7639    0.6854    25.685M  171.829G   \n2     resnet101   0.7441   0.6557    0.7645    0.6948    44.678M  327.412G   \n3     resnet152   0.7357   0.6573    0.7619    0.7027    60.321M  483.096G   \n4      vgg11_bn   0.7389   0.6635    0.7554    0.6839   132.997M  317.755G   \n5      vgg13_bn   0.7280   0.6510    0.7524    0.6776   133.182M  472.977G   \n6      vgg16_bn   0.7265   0.6588    0.7732    0.7089   138.494M  647.087G   \n7      vgg19_bn   0.7179   0.6322    0.7520    0.6792   143.807M  821.197G   \n8   densenet121   0.7419   0.6604    0.7647    0.6870     8.107M  119.767G   \n9   densenet169   0.7683   0.6980    0.7760    0.7074    14.278M  142.013G   \n10  densenet201   0.7372   0.6620    0.7616    0.7042    20.142M  181.420G   \n11  densenet161   0.7326   0.6526    0.7814    0.7042    28.809M  325.448G   \n\n   Time Consuming  \n0           5.42s  \n1           5.59s  \n2           7.02s  \n3           8.71s  \n4           5.85s  \n5            7.4s  \n6            8.0s  \n7           8.49s  \n8           6.26s  \n9            6.8s  \n10          7.93s  \n11          9.07s  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>val_auc</th>\n      <th>val_acc</th>\n      <th>test_auc</th>\n      <th>test_acc</th>\n      <th>parameters</th>\n      <th>FLOPS</th>\n      <th>Time Consuming</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>resnet34</td>\n      <td>0.7330</td>\n      <td>0.6635</td>\n      <td>0.7795</td>\n      <td>0.7089</td>\n      <td>21.926M</td>\n      <td>153.443G</td>\n      <td>5.42s</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>resnet50</td>\n      <td>0.7165</td>\n      <td>0.6541</td>\n      <td>0.7639</td>\n      <td>0.6854</td>\n      <td>25.685M</td>\n      <td>171.829G</td>\n      <td>5.59s</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>resnet101</td>\n      <td>0.7441</td>\n      <td>0.6557</td>\n      <td>0.7645</td>\n      <td>0.6948</td>\n      <td>44.678M</td>\n      <td>327.412G</td>\n      <td>7.02s</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>resnet152</td>\n      <td>0.7357</td>\n      <td>0.6573</td>\n      <td>0.7619</td>\n      <td>0.7027</td>\n      <td>60.321M</td>\n      <td>483.096G</td>\n      <td>8.71s</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>vgg11_bn</td>\n      <td>0.7389</td>\n      <td>0.6635</td>\n      <td>0.7554</td>\n      <td>0.6839</td>\n      <td>132.997M</td>\n      <td>317.755G</td>\n      <td>5.85s</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>vgg13_bn</td>\n      <td>0.7280</td>\n      <td>0.6510</td>\n      <td>0.7524</td>\n      <td>0.6776</td>\n      <td>133.182M</td>\n      <td>472.977G</td>\n      <td>7.4s</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>vgg16_bn</td>\n      <td>0.7265</td>\n      <td>0.6588</td>\n      <td>0.7732</td>\n      <td>0.7089</td>\n      <td>138.494M</td>\n      <td>647.087G</td>\n      <td>8.0s</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>vgg19_bn</td>\n      <td>0.7179</td>\n      <td>0.6322</td>\n      <td>0.7520</td>\n      <td>0.6792</td>\n      <td>143.807M</td>\n      <td>821.197G</td>\n      <td>8.49s</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>densenet121</td>\n      <td>0.7419</td>\n      <td>0.6604</td>\n      <td>0.7647</td>\n      <td>0.6870</td>\n      <td>8.107M</td>\n      <td>119.767G</td>\n      <td>6.26s</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>densenet169</td>\n      <td>0.7683</td>\n      <td>0.6980</td>\n      <td>0.7760</td>\n      <td>0.7074</td>\n      <td>14.278M</td>\n      <td>142.013G</td>\n      <td>6.8s</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>densenet201</td>\n      <td>0.7372</td>\n      <td>0.6620</td>\n      <td>0.7616</td>\n      <td>0.7042</td>\n      <td>20.142M</td>\n      <td>181.420G</td>\n      <td>7.93s</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>densenet161</td>\n      <td>0.7326</td>\n      <td>0.6526</td>\n      <td>0.7814</td>\n      <td>0.7042</td>\n      <td>28.809M</td>\n      <td>325.448G</td>\n      <td>9.07s</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_results = pd.read_csv(\"saved/result/pretrained_model_compare.csv\")\n",
    "pretrained_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrlll}\n",
      "\\toprule\n",
      "      model &  val\\_auc &  val\\_acc &  test\\_auc &  test\\_acc & parameters &    FLOPS & Time Consuming \\\\\n",
      "\\midrule\n",
      "   resnet34 &   0.7330 &   0.6635 &    0.7795 &    0.7089 &    21.926M & 153.443G &          5.42s \\\\\n",
      "   resnet50 &   0.7165 &   0.6541 &    0.7639 &    0.6854 &    25.685M & 171.829G &          5.59s \\\\\n",
      "  resnet101 &   0.7441 &   0.6557 &    0.7645 &    0.6948 &    44.678M & 327.412G &          7.02s \\\\\n",
      "  resnet152 &   0.7357 &   0.6573 &    0.7619 &    0.7027 &    60.321M & 483.096G &          8.71s \\\\\n",
      "   vgg11\\_bn &   0.7389 &   0.6635 &    0.7554 &    0.6839 &   132.997M & 317.755G &          5.85s \\\\\n",
      "   vgg13\\_bn &   0.7280 &   0.6510 &    0.7524 &    0.6776 &   133.182M & 472.977G &           7.4s \\\\\n",
      "   vgg16\\_bn &   0.7265 &   0.6588 &    0.7732 &    0.7089 &   138.494M & 647.087G &           8.0s \\\\\n",
      "   vgg19\\_bn &   0.7179 &   0.6322 &    0.7520 &    0.6792 &   143.807M & 821.197G &          8.49s \\\\\n",
      "densenet121 &   0.7419 &   0.6604 &    0.7647 &    0.6870 &     8.107M & 119.767G &          6.26s \\\\\n",
      "densenet169 &   0.7683 &   0.6980 &    0.7760 &    0.7074 &    14.278M & 142.013G &           6.8s \\\\\n",
      "densenet201 &   0.7372 &   0.6620 &    0.7616 &    0.7042 &    20.142M & 181.420G &          7.93s \\\\\n",
      "densenet161 &   0.7326 &   0.6526 &    0.7814 &    0.7042 &    28.809M & 325.448G &          9.07s \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pretrained_results.to_latex(index=False))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The performance of these three models is very similar.\n",
    "In general, for a deep structure model, the more deep network layers the model has, the more parameters and GPU resources are occupied, and the more time is consumed.\n",
    "But from the ACC and AUC results in Table above, a deeper model does not necessarily perform better.\n",
    "Therefore, the choice of a model needs to be considered based on the actual situation.\n",
    "For example, when a fast response is required, the faster running model should be prioritized, such as ResNet34,\n",
    "and when the memory resources are not so sufficient, DenseNet121 may be a considerable one.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# run the tensorboard to check the log of training loss and validation loss\n",
    "! tensorboard --logdir saved/log"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}